{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dipendenze",
   "id": "b9c511c85261d6fc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T05:51:26.680109Z",
     "start_time": "2025-04-29T05:51:25.104126Z"
    }
   },
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:51:26.704195Z",
     "start_time": "2025-04-29T05:51:26.690614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random_state_list = [int(i*11) for i in range(1,16)]\n",
    "#random_state_list = [123,42,456]\n",
    "ROW_TIME = 4 #secondi di dati riassunti in una riga del dataframe\n",
    "\n",
    "\n",
    "mypath = os.getcwd() + '/data/data_total/'\n",
    "os.makedirs(mypath[:-1], exist_ok=True)"
   ],
   "id": "42ed738a3e9007",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Carica Dati per Modello",
   "id": "25e0101cca2f249e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.207886Z",
     "start_time": "2025-04-28T17:18:23.255920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mypath_carica = os.getcwd() + '/data/Processed_data/'\n",
    "file_pattern = 'grouped_data.*'\n",
    "\n",
    "file_list = [\n",
    "    f for f in listdir(mypath_carica)\n",
    "    if (isfile(join(mypath_carica, f)) and\n",
    "               re.compile(file_pattern).match(f))]\n",
    "df_data = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(mypath_carica + file, header=0).iloc[:,1:]\n",
    "    df_data = pd.concat([df_data, df]).reset_index(drop=True)\n",
    "def set_labels(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['Activity'])\n",
    "    return df, label_encoder.classes_\n",
    "df_data, labels = set_labels(df_data)"
   ],
   "id": "c470b4b68821ef3b",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bilanciamento Dati\n",
    "Tutti gli utenti avranno lo stesso numero di dati di quello che ne ha meno"
   ],
   "id": "2eac552c33d6fb10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.452163Z",
     "start_time": "2025-04-28T17:18:24.230621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def balance_user_labels(df, random_state=42):\n",
    "    min_count = df.groupby(['Userid', 'label', 'position']).size().min()\n",
    "\n",
    "    def sample_group(group):\n",
    "        return group.sample(n=min_count, random_state=random_state)\n",
    "\n",
    "    balanced_df = df.groupby(['Userid', 'label', 'position']).apply(sample_group).reset_index(drop=True)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "df_data = balance_user_labels(df_data)"
   ],
   "id": "f1873bdc9f5e97c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\AppData\\Local\\Temp\\ipykernel_3120\\1720084609.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = df.groupby(['Userid', 'label', 'position']).apply(sample_group).reset_index(drop=True)\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Per quanto uso multipli sensori",
   "id": "ff1a97f2a8b6aa7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.500880Z",
     "start_time": "2025-04-28T17:18:24.494656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_features_for_each_sensor(df_data, positions):\n",
    "    df_final = pd.DataFrame()  # This will be the final DataFrame including features and labels\n",
    "    labels_columns = []  # To keep track of the names of the 'label' columns for each position\n",
    "\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "\n",
    "    for position in positions:\n",
    "        # Prepare feature columns for the current position\n",
    "        df_data_pos = df_data[df_data['position'] == position].drop(columns=['label', 'position']).rename(\n",
    "                columns=lambda x: x + '_' + position).reset_index(drop=True)\n",
    "\n",
    "        # Prepare label column for the current position\n",
    "        label_col_name = f'label_{position}'\n",
    "        df_labels = df_data[df_data['position'] == position]['label'].reset_index(drop=True).to_frame(name=label_col_name)\n",
    "        labels_columns.append(label_col_name)\n",
    "\n",
    "        # Concatenate feature and label columns\n",
    "        df_combined = pd.concat([df_data_pos, df_labels], axis=1)\n",
    "        df_final = pd.concat([df_final, df_combined], axis=1)\n",
    "    # Filter rows where all label columns have the same value\n",
    "    mask = df_final.apply(lambda row: all(row[col] == row[labels_columns[0]] for col in labels_columns), axis=1)\n",
    "    #first_label_col = labels_columns[0]\n",
    "    #mask = df_final.apply(lambda row: all(row.to_dict()[col] == row.to_dict()[first_label_col] for col in labels_columns), axis=1)\n",
    "    df_filtered = df_final[mask]\n",
    "    # Optionally, you might want to drop redundant label columns and keep just one\n",
    "    df_filtered = df_filtered.drop(columns=labels_columns[1:]).rename(columns={labels_columns[0]: 'label'})\n",
    "\n",
    "    return df_filtered.dropna()"
   ],
   "id": "7993259c0dd147f0",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.537526Z",
     "start_time": "2025-04-28T17:18:24.532354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def duplicaRighePesi(df_moved, weight, varianza):\n",
    "    if not varianza:\n",
    "        df_moved = df_moved.loc[np.repeat(df_moved.index, int(weight))].reset_index(drop=True)\n",
    "    elif varianza and weight > 1:\n",
    "        df_moved['is_original'] = True\n",
    "        repeated_part = df_moved.loc[np.repeat(df_moved.index, int(weight) - 1)].copy()\n",
    "        repeated_part['is_original'] = False\n",
    "        df_moved = pd.concat([df_moved, repeated_part], ignore_index=True)\n",
    "\n",
    "        feature_cols = df_moved.columns.difference(['label', 'is_original'])\n",
    "        feature_cols = df_moved[feature_cols].select_dtypes(include=[np.number]).columns\n",
    "        df_moved[feature_cols] = df_moved[feature_cols].astype(float)\n",
    "\n",
    "        df_moved.loc[~df_moved['is_original'], feature_cols] *= np.random.uniform(0.99, 1.01, size=df_moved.loc[~df_moved['is_original'], feature_cols].shape)\n",
    "\n",
    "        df_moved = df_moved.drop('is_original', axis=1)\n",
    "\n",
    "    return df_moved\n"
   ],
   "id": "dca15d1bea14f160",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.555157Z",
     "start_time": "2025-04-28T17:18:24.539861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_test_data(df_data, user=None, random_state=42, percentage=None, weight = None, varianza = False):\n",
    "    positions = list(df_data['position'].unique())\n",
    "    all_features = [item for item in df_data.columns if\n",
    "                    item not in ['Timestamp', 'Userid', 'UserAge', 'UserSex', 'UserHeight', 'UserWeight', 'Activity',\n",
    "                                 'position', 'label', 'MagnxEnergy', 'MagnyEnergy', 'MagnzEnergy', 'MagnMagnitude',\n",
    "                                 'MagnMagnitudeMean', 'MagnMagnitudeMin', 'MagnMagnitudeMax', 'MagnMagnitudeStd',\n",
    "                                 'MagnMagnitudeEnergy']]\n",
    "    all_features = [item for item in all_features if not re.match(r'.*MagnMagnitude.*', item)]\n",
    "    magnitude_features = [item for item in all_features if re.match(r'.*Magnitude.*', item)]\n",
    "    features = magnitude_features\n",
    "\n",
    "\n",
    "    df_data = df_data[df_data['position'].isin(positions)]\n",
    "    for position in positions:\n",
    "        if position not in list(df_data['position'].unique()):\n",
    "            print(f'Position {position} not found in the dataset')\n",
    "            return None\n",
    "\n",
    "    df_train = df_data[df_data['Userid'] != user].reset_index(drop=True)\n",
    "    df_test = df_data[df_data['Userid'] == user].reset_index(drop=True)\n",
    "    df_test, df_testFISSO = train_test_split(df_test, test_size=0.2, random_state=random_state,stratify=df_test['label'])  # 80/20 split\n",
    "\n",
    "    #sposto le righe\n",
    "    moved_indices = []\n",
    "    for label_value in df_test['label'].unique():\n",
    "        df_test_label = df_test[df_test['label'] == label_value]\n",
    "        for position_value in df_test_label['position'].unique():\n",
    "            df_test_label_position = df_test_label[df_test_label['position'] == position_value]\n",
    "            num_to_move = int(len(df_test_label_position) * percentage)\n",
    "            if num_to_move > 0:\n",
    "                indices_to_move = df_test_label_position.sample(n=num_to_move, random_state=random_state).index.tolist()\n",
    "                moved_indices.extend(indices_to_move)\n",
    "\n",
    "    righe_mosse = len(moved_indices)\n",
    "\n",
    "    if len(positions) > 1:\n",
    "        righe_mosse = righe_mosse / len(positions)\n",
    "        df_train = get_features_for_each_sensor(df_train[features + ['position', 'label']], positions)\n",
    "        df_testFISSO  = get_features_for_each_sensor(df_testFISSO[features + ['position', 'label']], positions)\n",
    "\n",
    "    if moved_indices:\n",
    "        df_moved = df_test.loc[moved_indices].copy()\n",
    "        if len(positions) > 1:\n",
    "            df_moved = get_features_for_each_sensor(df_moved[features + ['position', 'label']], positions)\n",
    "        df_moved = duplicaRighePesi(df_moved, weight, varianza)\n",
    "        df_train = pd.concat([df_train, df_moved], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    if len(positions) > 1:\n",
    "        X_train = df_train.drop(columns=['label'])\n",
    "        X_test = df_testFISSO.drop(columns=['label'])\n",
    "    else:\n",
    "        X_train = df_train[features]\n",
    "        X_test = df_testFISSO[features]\n",
    "    y_train = df_train['label']\n",
    "    y_test  = df_testFISSO['label']\n",
    "\n",
    "    #print(\"ed io gli passo:\",moved_indices)\n",
    "    return X_train, X_test, y_train, y_test, righe_mosse"
   ],
   "id": "a3a77f0adb4c6944",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Alleno Modello",
   "id": "c2d22b2daf9332c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.582975Z",
     "start_time": "2025-04-28T17:18:24.577535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(X_train, X_test, y_train, y_test, random_state):\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    #accuracy = xgb.score(X_test, y_test)\n",
    "    #return accuracy, y_pred\n",
    "    return y_pred"
   ],
   "id": "c61be4943ea4b52a",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Divido i dati",
   "id": "f948cce5c376ef3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.613655Z",
     "start_time": "2025-04-28T17:18:24.602204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def k_fold_cross_validation(position, df_data, weight_list=None, varianza = False, lista_percentuali = None):\n",
    "    global df_f1_score\n",
    "    all_sensors = len(position) > 1\n",
    "    labels = df_data['Activity'].unique()\n",
    "\n",
    "    if weight_list is None:\n",
    "        weight_list = [5, 10, 25, 50, 75, 100, 250, 500, 750, 1000, 1500, 2000, 2500, 5000, 7500, 10000]\n",
    "    if lista_percentuali is None:\n",
    "        lista_percentuali = [i / 100 for i in range(0, 101, 5)]\n",
    "\n",
    "    for rand_state in random_state_list:\n",
    "        print(\" random state:\", rand_state)\n",
    "        for peso in weight_list:\n",
    "            for percentuale_nuovo_train in lista_percentuali:\n",
    "                #print(\",\".join(position)+\" stato \"+str(rand_state)+\" \"+str(peso)+\"w \"+str(int(percentuale_nuovo_train*100))+\"%\")\n",
    "                for k in df_data['Userid'].unique():\n",
    "                    X_train, X_test, y_train, y_test, num_dati_spostati = get_train_test_data(df_data[df_data['position'].isin(position)], user=k, random_state = rand_state, percentage=float(percentuale_nuovo_train), weight = float(peso), varianza = varianza)\n",
    "\n",
    "                    start = time.perf_counter()\n",
    "                    y_pred = train_model(X_train, X_test, y_train, y_test,random_state = rand_state)\n",
    "                    end = time.perf_counter()\n",
    "                    durata = end - start\n",
    "\n",
    "                    class_report = classification_report(y_test, y_pred, output_dict=True,zero_division=0)\n",
    "                    for label_idx, label in zip(y_train.unique(), labels):\n",
    "                        df = pd.DataFrame()\n",
    "                        df['label'] = [label]\n",
    "                        df['timeUsed'] = [num_dati_spostati * ROW_TIME]\n",
    "                        df['percentage'] = [int(percentuale_nuovo_train*100)]\n",
    "                        df['weight'] = [float(peso)]\n",
    "                        df['time'] = [round(durata, 2)]\n",
    "                        df['randomState'] = [rand_state]\n",
    "                        df['position'] = ['both sensors'] if all_sensors else position\n",
    "\n",
    "                        key_formats = [str(label_idx), str(float(label_idx)), str(int(label_idx))]\n",
    "                        for key in key_formats:\n",
    "                            try:\n",
    "                                df['f1-score'] = [class_report[key]['f1-score']]\n",
    "                                df['precision'] = [class_report[key]['precision']]\n",
    "                                df['recall'] = [class_report[key]['recall']]\n",
    "                                break\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "\n",
    "                        df_f1_score = pd.concat([df_f1_score, df], axis=0).reset_index(drop=True)\n",
    "\n",
    "                if baseCalcolata:\n",
    "                    if all_sensors or position[0] in ['left wrist', 'right pocket']:\n",
    "                        df_appena_calcolato = df_f1_score[df_f1_score['weight'] == peso]\n",
    "                        df_appena_calcolato = df_appena_calcolato[df_appena_calcolato['percentage'] == int(percentuale_nuovo_train*100)]\n",
    "\n",
    "                        if all_sensors:\n",
    "                            pos_key = 'both sensors'\n",
    "                            f1_s_max = f1_s_max_both\n",
    "                        else:\n",
    "                            pos_key = position[0]\n",
    "                            f1_s_max = f1_s_max_lw if pos_key == 'left wrist' else f1_s_max_rp\n",
    "\n",
    "                        f1_s_mifermo = prendiMax(df_appena_calcolato, pos_key, rand_state)\n",
    "                        #print(f\"confronto {f1_s_mifermo[rand_state]} e {f1_s_max[rand_state]}\")\n",
    "\n",
    "                        if f1_s_mifermo[rand_state] >= f1_s_max[rand_state]:\n",
    "                            print(f\"  stop a {int(percentuale_nuovo_train * 100)}%({num_dati_spostati * ROW_TIME}) per peso {peso} \"\n",
    "                                  f\"in quanto {f1_s_mifermo[rand_state]} è maggiore del max a peso 1 ({f1_s_max[rand_state]})\")\n",
    "                            break"
   ],
   "id": "52788d4b567a595",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.640377Z",
     "start_time": "2025-04-28T17:18:24.633602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prendiMax(df, position, random_states):\n",
    "    if not isinstance(random_states, list):\n",
    "        random_states = [random_states]\n",
    "\n",
    "    df_pos = df[(df['position'] == position) & (df['randomState'].isin(random_states))]\n",
    "    grouped = df_pos.groupby(['randomState', 'timeUsed'])['f1-score'].mean()\n",
    "    max_medie = grouped.groupby('randomState').max()\n",
    "\n",
    "    return max_medie.to_dict()\n"
   ],
   "id": "206ef69644e0055b",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Caso Base\n",
    "Peso 1, fa da ottimizzatore per i veri modelli con tutti i vari pesi facendoli fermare quando superano il massimo di questo"
   ],
   "id": "8a36f0b922f7056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:48:11.254152Z",
     "start_time": "2025-04-28T17:18:24.665728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseCalcolata = False\n",
    "\n",
    "df_f1_score = pd.DataFrame()\n",
    "\n",
    "# CREO CASO BASE\n",
    "print(\"both sensors\")\n",
    "k_fold_cross_validation(['right pocket','left wrist'], df_data, weight_list=[1])\n",
    "print(\"right pocket\")\n",
    "k_fold_cross_validation(['right pocket'], df_data, weight_list=[1])\n",
    "print(\"left wrist\")\n",
    "k_fold_cross_validation(['left wrist'], df_data, weight_list=[1])\n",
    "df_base = df_f1_score[df_f1_score['weight'] == 1] #opzionale in teoria\n",
    "\n",
    "f1_s_max_both = prendiMax(df_base, 'both sensors',random_state_list)\n",
    "f1_s_max_lw = prendiMax(df_base, 'left wrist',random_state_list)\n",
    "f1_s_max_rp = prendiMax(df_base, 'right pocket',random_state_list)\n",
    "\n",
    "print(\"f1-score both sensors:\",f1_s_max_both)\n",
    "print(\"f1-score left wrist:\",f1_s_max_lw)\n",
    "print(\"f1-score right pocket:\",f1_s_max_rp)\n",
    "\n",
    "pesoBaseData = df_f1_score.copy()\n",
    "\n",
    "baseCalcolata = True"
   ],
   "id": "a56dc366ad20fbe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both sensors\n",
      " random state: 123\n",
      " random state: 42\n",
      " random state: 456\n",
      "right pocket\n",
      " random state: 123\n",
      " random state: 42\n",
      " random state: 456\n",
      "left wrist\n",
      " random state: 123\n",
      " random state: 42\n",
      " random state: 456\n",
      "f1-score both sensors: {42: 0.9572294372294372, 123: 0.9722698412698412, 456: 0.9668571428571429}\n",
      "f1-score left wrist: {42: 0.9074839633327489, 123: 0.911588410193702, 456: 0.9146511517407228}\n",
      "f1-score right pocket: {42: 0.9285193957229311, 123: 0.9278508869806269, 456: 0.9165150598715056}\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:48:11.419964Z",
     "start_time": "2025-04-28T17:48:11.337037Z"
    }
   },
   "cell_type": "code",
   "source": "pesoBaseData.to_csv(mypath + 'modelXGBtotal_baseline.csv')",
   "id": "5168e96f706fd1e9",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:03:47.576408Z",
     "start_time": "2025-04-28T17:48:11.436135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_f1_score = pd.DataFrame()\n",
    "\n",
    "print(\"both sensors\")\n",
    "k_fold_cross_validation(['right pocket','left wrist'], df_data)\n",
    "print(\"right pocket\")\n",
    "k_fold_cross_validation(['right pocket'], df_data)\n",
    "print(\"left wrist\")\n",
    "k_fold_cross_validation(['left wrist'], df_data)\n",
    "\n",
    "baseData = df_f1_score.copy()\n",
    "baseData = pd.concat([baseData, pesoBaseData]).reset_index(drop=True)\n"
   ],
   "id": "9521b5a040138e4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both sensors\n",
      " random state: 123\n",
      "  stop a 90%(1606.0) per peso 5 in quanto 0.9772698412698412 è maggiore del max a peso 1 (0.9722698412698412)\n",
      " random state: 42\n",
      "  stop a 80%(1426.0) per peso 5 in quanto 0.9720865800865801 è maggiore del max a peso 1 (0.9572294372294372)\n",
      "  stop a 65%(1156.0) per peso 10 in quanto 0.9586017316017316 è maggiore del max a peso 1 (0.9572294372294372)\n",
      "  stop a 50%(890.0) per peso 25 in quanto 0.9666262626262626 è maggiore del max a peso 1 (0.9572294372294372)\n",
      " random state: 456\n",
      "  stop a 90%(1606.0) per peso 25 in quanto 0.9683174603174604 è maggiore del max a peso 1 (0.9668571428571429)\n",
      "right pocket\n",
      " random state: 123\n",
      "  stop a 65%(1152) per peso 5 in quanto 0.9314217117550956 è maggiore del max a peso 1 (0.9278508869806269)\n",
      "  stop a 60%(1072) per peso 10 in quanto 0.9286873215702991 è maggiore del max a peso 1 (0.9278508869806269)\n",
      "  stop a 65%(1152) per peso 25 in quanto 0.9347396917215481 è maggiore del max a peso 1 (0.9278508869806269)\n",
      " random state: 42\n",
      "  stop a 80%(1432) per peso 5 in quanto 0.9285666218464024 è maggiore del max a peso 1 (0.9285193957229311)\n",
      "  stop a 75%(1332) per peso 10 in quanto 0.931487147212218 è maggiore del max a peso 1 (0.9285193957229311)\n",
      "  stop a 60%(1072) per peso 25 in quanto 0.9286377228919649 è maggiore del max a peso 1 (0.9285193957229311)\n",
      " random state: 456\n",
      "  stop a 80%(1432) per peso 5 in quanto 0.9186563245948551 è maggiore del max a peso 1 (0.9165150598715056)\n",
      "  stop a 80%(1432) per peso 10 in quanto 0.9220598429818142 è maggiore del max a peso 1 (0.9165150598715056)\n",
      "  stop a 70%(1240) per peso 25 in quanto 0.9185536272523046 è maggiore del max a peso 1 (0.9165150598715056)\n",
      "left wrist\n",
      " random state: 123\n",
      "  stop a 85%(1512) per peso 5 in quanto 0.915141821845792 è maggiore del max a peso 1 (0.911588410193702)\n",
      "  stop a 85%(1512) per peso 10 in quanto 0.9187610373539228 è maggiore del max a peso 1 (0.911588410193702)\n",
      "  stop a 80%(1432) per peso 25 in quanto 0.9150393762283698 è maggiore del max a peso 1 (0.911588410193702)\n",
      " random state: 42\n",
      "  stop a 80%(1432) per peso 5 in quanto 0.9101518798071061 è maggiore del max a peso 1 (0.9074839633327489)\n",
      "  stop a 80%(1432) per peso 10 in quanto 0.9109912013658391 è maggiore del max a peso 1 (0.9074839633327489)\n",
      "  stop a 60%(1072) per peso 25 in quanto 0.9075516082603933 è maggiore del max a peso 1 (0.9074839633327489)\n",
      " random state: 456\n",
      "  stop a 85%(1512) per peso 5 in quanto 0.9206289443113448 è maggiore del max a peso 1 (0.9146511517407228)\n",
      "  stop a 80%(1432) per peso 10 in quanto 0.920118272025085 è maggiore del max a peso 1 (0.9146511517407228)\n",
      "  stop a 75%(1332) per peso 25 in quanto 0.9189570290138975 è maggiore del max a peso 1 (0.9146511517407228)\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Salvo modello base",
   "id": "5f646e90baccb225"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:03:47.781390Z",
     "start_time": "2025-04-28T19:03:47.635788Z"
    }
   },
   "cell_type": "code",
   "source": "baseData.to_csv(mypath + 'modelXGBtotal_base.csv')",
   "id": "2114d7384a8af855",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Calcolo modello con varianza\n",
    "Varianza definita come il moltiplicare ogni riga ripetuta per un valore compreso tra 0.99 e 1.01"
   ],
   "id": "6041785e8693b551"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:20:15.736753Z",
     "start_time": "2025-04-28T19:03:47.782481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_f1_score = pd.DataFrame()\n",
    "\n",
    "print(\"both sensors\")\n",
    "k_fold_cross_validation(['right pocket','left wrist'], df_data, varianza = True)\n",
    "print(\"right pocket\")\n",
    "k_fold_cross_validation(['right pocket'], df_data, varianza = True)\n",
    "print(\"left wrist\")\n",
    "k_fold_cross_validation(['left wrist'], df_data, varianza = True)\n",
    "\n",
    "varianzaData = df_f1_score.copy()\n",
    "varianzaData = pd.concat([varianzaData, pesoBaseData]).reset_index(drop=True)"
   ],
   "id": "2b187c8c5b769b68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both sensors\n",
      " random state: 123\n",
      "  stop a 90%(1606.0) per peso 5 in quanto 0.9766349206349205 è maggiore del max a peso 1 (0.9722698412698412)\n",
      " random state: 42\n",
      "  stop a 60%(1068.0) per peso 5 in quanto 0.9608888888888889 è maggiore del max a peso 1 (0.9572294372294372)\n",
      "  stop a 60%(1068.0) per peso 10 in quanto 0.9583246753246754 è maggiore del max a peso 1 (0.9572294372294372)\n",
      "  stop a 40%(710.0) per peso 25 in quanto 0.9576103896103896 è maggiore del max a peso 1 (0.9572294372294372)\n",
      " random state: 456\n",
      "  stop a 90%(1606.0) per peso 5 in quanto 0.9683174603174604 è maggiore del max a peso 1 (0.9668571428571429)\n",
      "right pocket\n",
      " random state: 123\n",
      "  stop a 70%(1240) per peso 5 in quanto 0.9293238781393016 è maggiore del max a peso 1 (0.9278508869806269)\n",
      "  stop a 75%(1332) per peso 10 in quanto 0.934662163699467 è maggiore del max a peso 1 (0.9278508869806269)\n",
      "  stop a 65%(1152) per peso 25 in quanto 0.932350894148607 è maggiore del max a peso 1 (0.9278508869806269)\n",
      " random state: 42\n",
      "  stop a 60%(1072) per peso 5 in quanto 0.9287086700771094 è maggiore del max a peso 1 (0.9285193957229311)\n",
      "  stop a 70%(1240) per peso 10 in quanto 0.9337517228164627 è maggiore del max a peso 1 (0.9285193957229311)\n",
      "  stop a 55%(972) per peso 25 in quanto 0.9285326565788674 è maggiore del max a peso 1 (0.9285193957229311)\n",
      " random state: 456\n",
      "  stop a 70%(1240) per peso 5 in quanto 0.9220571478350079 è maggiore del max a peso 1 (0.9165150598715056)\n",
      "  stop a 70%(1240) per peso 10 in quanto 0.9234206196255006 è maggiore del max a peso 1 (0.9165150598715056)\n",
      "  stop a 65%(1152) per peso 25 in quanto 0.916649701167423 è maggiore del max a peso 1 (0.9165150598715056)\n",
      "left wrist\n",
      " random state: 123\n",
      "  stop a 95%(1692) per peso 5 in quanto 0.9232634384071124 è maggiore del max a peso 1 (0.911588410193702)\n",
      "  stop a 90%(1612) per peso 10 in quanto 0.9118517331344728 è maggiore del max a peso 1 (0.911588410193702)\n",
      "  stop a 80%(1432) per peso 25 in quanto 0.9186867630522723 è maggiore del max a peso 1 (0.911588410193702)\n",
      " random state: 42\n",
      "  stop a 85%(1512) per peso 5 in quanto 0.918418016850711 è maggiore del max a peso 1 (0.9074839633327489)\n",
      "  stop a 55%(972) per peso 10 in quanto 0.9104283031789341 è maggiore del max a peso 1 (0.9074839633327489)\n",
      "  stop a 55%(972) per peso 25 in quanto 0.9079476377797008 è maggiore del max a peso 1 (0.9074839633327489)\n",
      " random state: 456\n",
      "  stop a 80%(1432) per peso 5 in quanto 0.9153257804173925 è maggiore del max a peso 1 (0.9146511517407228)\n",
      "  stop a 65%(1152) per peso 10 in quanto 0.9164193472158141 è maggiore del max a peso 1 (0.9146511517407228)\n",
      "  stop a 70%(1240) per peso 25 in quanto 0.9151371283987352 è maggiore del max a peso 1 (0.9146511517407228)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Salvo modello con varianza",
   "id": "802a20fb1f25bed8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:20:16.009356Z",
     "start_time": "2025-04-28T20:20:15.818596Z"
    }
   },
   "cell_type": "code",
   "source": "varianzaData.to_csv(mypath + 'modelXGBtotal_varianza.csv')",
   "id": "5058f1c732b11a41",
   "outputs": [],
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

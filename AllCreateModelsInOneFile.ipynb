{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c511c85261d6fc",
   "metadata": {},
   "source": [
    "### Dipendenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:51:26.680109Z",
     "start_time": "2025-04-29T05:51:25.104126Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed738a3e9007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:51:26.704195Z",
     "start_time": "2025-04-29T05:51:26.690614Z"
    }
   },
   "outputs": [],
   "source": [
    "#RANDOM_STATE_LIST = [int(i*11) for i in range(1,16)]\n",
    "RANDOM_STATE_LIST = [42,123,456]\n",
    "ROW_TIME = 4 #secondi di dati riassunti in una riga del dataframe\n",
    "SAVE = False\n",
    "\n",
    "mypath = os.getcwd() + '/data/data_total/'\n",
    "os.makedirs(mypath[:-1], exist_ok=True)\n",
    "\n",
    "DATASET = 'MultiPositionWearable' #'MultiPositionWearable', 'selfBACK', '?'\n",
    "\n",
    "if DATASET == 'MultiPositionWearable':\n",
    "  NOMI_FILE = {\n",
    "    'baseline': 'MultiPosition_wearable_modelXGBtotal_baseline',\n",
    "    'modello_base': 'MultiPosition_wearable_modelXGBtotal_base',\n",
    "    'modello_varianza': 'MultiPosition_wearable_modelXGBtotal_varianza',\n",
    "    'cartella_dati': 'MultiPosition_wearable_processed_data'\n",
    "  }\n",
    "elif DATASET == 'selfBACK':\n",
    "  NOMI_FILE = {\n",
    "    'baseline': 'selfBACK_modelXGBtotal_baseline',\n",
    "    'modello_base': 'selfBACK_modelXGBtotal_base',\n",
    "    'modello_varianza': 'selfBACK_modelXGBtotal_varianza',\n",
    "    'cartella_dati': 'selfBACK_processed_data'\n",
    "  }\n",
    "elif DATASET == '?':\n",
    "  NOMI_FILE = {\n",
    "    'baseline': '?_modelXGBtotal_baseline',\n",
    "    'modello_base': '?_modelXGBtotal_base',\n",
    "    'modello_varianza': '?_modelXGBtotal_varianza',\n",
    "    'cartella_dati': '?_processed_data'\n",
    "  }\n",
    "else:\n",
    "    raise ValueError(\"DATASET non valido. Scegliere tra 'MultiPositionWearable', 'selfBACK' o '?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0101cca2f249e",
   "metadata": {},
   "source": [
    "### Carica Dati per Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470b4b68821ef3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.207886Z",
     "start_time": "2025-04-28T17:18:23.255920Z"
    }
   },
   "outputs": [],
   "source": [
    "mypath_carica = os.getcwd() + '/data/' + NOMI_FILE['cartella_dati'] + '/'\n",
    "file_pattern = 'grouped_data.*'\n",
    "\n",
    "file_list = [\n",
    "    f for f in listdir(mypath_carica)\n",
    "    if (isfile(join(mypath_carica, f)) and\n",
    "               re.compile(file_pattern).match(f))]\n",
    "df_data = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(mypath_carica + file, header=0).iloc[:,1:] if DATASET == 'default' else pd.read_csv(mypath_carica + file, header=0)\n",
    "    df_data = pd.concat([df_data, df]).reset_index(drop=True)\n",
    "def set_labels(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['Activity'])\n",
    "    return df, label_encoder.classes_\n",
    "df_data, labels = set_labels(df_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac552c33d6fb10",
   "metadata": {},
   "source": [
    "### Bilanciamento Dati\n",
    "Tutti gli utenti avranno lo stesso numero di dati di quello che ne ha meno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1873bdc9f5e97c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.452163Z",
     "start_time": "2025-04-28T17:18:24.230621Z"
    }
   },
   "outputs": [],
   "source": [
    "def balance_user_labels(df):\n",
    "    min_count = df.groupby(['Userid', 'position', 'label']).size().min()\n",
    "\n",
    "    def sample_group(group):\n",
    "        return group.head(min_count)\n",
    "\n",
    "    balanced_df = df.groupby(['Userid', 'position', 'label']).apply(sample_group).reset_index(drop=True)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "df_data = balance_user_labels(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_Spaces(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "df_data = strip_Spaces(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a97f2a8b6aa7d",
   "metadata": {},
   "source": [
    "### Per quanto uso multipli sensori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993259c0dd147f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.500880Z",
     "start_time": "2025-04-28T17:18:24.494656Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features_for_each_sensor(df_data, positions):\n",
    "    df_final = pd.DataFrame()  # This will be the final DataFrame including features and labels\n",
    "    labels_columns = []  # To keep track of the names of the 'label' columns for each position\n",
    "\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "\n",
    "    for position in positions:\n",
    "        # Prepare feature columns for the current position\n",
    "        df_data_pos = df_data[df_data['position'] == position].drop(columns=['label', 'position']).rename(\n",
    "            columns=lambda x: x + '_' + position).reset_index(drop=True)\n",
    "\n",
    "        # Prepare label column for the current position\n",
    "        label_col_name = f'label_{position}'\n",
    "        df_labels = df_data[df_data['position'] == position]['label'].reset_index(drop=True).to_frame(name=label_col_name)\n",
    "        labels_columns.append(label_col_name)\n",
    "        # Concatenate feature and label columns\n",
    "        df_combined = pd.concat([df_data_pos, df_labels], axis=1)\n",
    "        df_final = pd.concat([df_final, df_combined], axis=1)\n",
    "\n",
    "    # Filter rows where all label columns have the same value\n",
    "    mask = df_final.apply(lambda row: all(row[col] == row[labels_columns[0]] for col in labels_columns), axis=1)\n",
    "    df_filtered = df_final[mask]\n",
    "\n",
    "    # Optionally, you might want to drop redundant label columns and keep just one\n",
    "    df_filtered = df_filtered.drop(columns=labels_columns[1:]).rename(columns={labels_columns[0]: 'label'})\n",
    "\n",
    "    return df_filtered.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca15d1bea14f160",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.537526Z",
     "start_time": "2025-04-28T17:18:24.532354Z"
    }
   },
   "outputs": [],
   "source": [
    "def duplicaRighePesi(df_moved, weight, varianza):\n",
    "    if not varianza:\n",
    "        df_moved = df_moved.loc[np.repeat(df_moved.index, int(weight))].reset_index(drop=True)\n",
    "    elif varianza and weight > 1:\n",
    "        df_moved['is_original'] = True\n",
    "        repeated_part = df_moved.loc[np.repeat(df_moved.index, int(weight) - 1)].copy()\n",
    "        repeated_part['is_original'] = False\n",
    "        df_moved = pd.concat([df_moved, repeated_part], ignore_index=True)\n",
    "\n",
    "        feature_cols = df_moved.columns.difference(['label', 'is_original'])\n",
    "        feature_cols = df_moved[feature_cols].select_dtypes(include=[np.number]).columns\n",
    "        df_moved[feature_cols] = df_moved[feature_cols].astype(float)\n",
    "\n",
    "        df_moved.loc[~df_moved['is_original'], feature_cols] *= np.random.uniform(0.99, 1.01, size=df_moved.loc[~df_moved['is_original'], feature_cols].shape)\n",
    "\n",
    "        df_moved = df_moved.drop('is_original', axis=1)\n",
    "\n",
    "    return df_moved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a77f0adb4c6944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.555157Z",
     "start_time": "2025-04-28T17:18:24.539861Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test_data(df_data, user=None, random_state=42, percentage=None, weight = None, varianza = False):\n",
    "    all_features = [item for item in df_data.columns if\n",
    "                    item not in ['Timestamp', 'Userid', 'UserAge', 'UserSex', 'UserHeight', 'UserWeight', 'Activity',\n",
    "                                 'position', 'label', 'MagnxEnergy', 'MagnyEnergy', 'MagnzEnergy', 'MagnMagnitude',\n",
    "                                 'MagnMagnitudeMean', 'MagnMagnitudeMin', 'MagnMagnitudeMax', 'MagnMagnitudeStd',\n",
    "                                 'MagnMagnitudeEnergy']]\n",
    "    all_features = [item for item in all_features if not re.match(r'.*MagnMagnitude.*', item)]\n",
    "    features = [item for item in all_features if re.match(r'.*Magnitude.*', item)]\n",
    "    # ALLA FINE USO SOLO MAGNITUDI DI ACC E GYRO\n",
    "    positions = list(df_data['position'].unique())\n",
    "    df_train = df_data[df_data['Userid'] != user].reset_index(drop=True)\n",
    "    df_test = df_data[df_data['Userid'] == user].reset_index(drop=True)\n",
    "\n",
    "    df_test_list = []\n",
    "    df_testFISSO_list = []\n",
    "\n",
    "    for (label, position), group in df_test.groupby(['label', 'position']):\n",
    "        split_idx = int(len(group) * 0.8)\n",
    "        group = group.sort_index()\n",
    "        df_test_list.append(group.iloc[:split_idx])\n",
    "        df_testFISSO_list.append(group.iloc[split_idx:])\n",
    "\n",
    "    df_test = pd.concat(df_test_list).reset_index(drop=True)\n",
    "    df_testFISSO = pd.concat(df_testFISSO_list).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #sposto le righe\n",
    "    moved_indices = []\n",
    "    for label_value in df_test['label'].unique():\n",
    "        df_test_label = df_test[df_test['label'] == label_value]\n",
    "        for position_value in df_test_label['position'].unique():\n",
    "            df_test_label_position = df_test_label[df_test_label['position'] == position_value]\n",
    "            num_to_move = int(len(df_test_label_position) * percentage)\n",
    "            if num_to_move > 0:\n",
    "                indices_to_move = df_test_label_position.sample(n=num_to_move, random_state=random_state).index.tolist()\n",
    "                moved_indices.extend(indices_to_move)\n",
    "    righe_mosse = len(moved_indices)\n",
    "\n",
    "    if len(positions) > 1:\n",
    "        righe_mosse = righe_mosse / len(positions)\n",
    "        df_train = get_features_for_each_sensor(df_train[features + ['position', 'label']], positions)\n",
    "        df_testFISSO  = get_features_for_each_sensor(df_testFISSO[features + ['position', 'label']], positions)\n",
    "        print(\"fisse \",df_testFISSO['label'].value_counts())\n",
    "\n",
    "\n",
    "    if moved_indices:\n",
    "        df_moved = df_test.loc[moved_indices].copy()\n",
    "        if len(positions) > 1:\n",
    "            df_moved = get_features_for_each_sensor(df_moved[features + ['position', 'label']], positions)\n",
    "            print(\"usabili \",df_moved['label'].value_counts())\n",
    "        df_moved = duplicaRighePesi(df_moved, weight, varianza)\n",
    "        df_train = pd.concat([df_train, df_moved], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    if len(positions) > 1:\n",
    "        X_train = df_train.drop(columns=['label'])\n",
    "        X_test = df_testFISSO.drop(columns=['label'])\n",
    "    else:\n",
    "        X_train = df_train[features]\n",
    "        X_test = df_testFISSO[features]\n",
    "    y_train = df_train['label']\n",
    "    y_test  = df_testFISSO['label']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, righe_mosse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prendiMax(df, position, random_states):\n",
    "    if not isinstance(random_states, list):\n",
    "        random_states = [random_states]\n",
    "\n",
    "    df_pos = df[(df['position'] == position) & (df['randomState'].isin(random_states))]\n",
    "    grouped = df_pos.groupby(['randomState', 'timeUsed'])['f1-score'].mean()\n",
    "    max_medie = grouped.groupby('randomState').max()\n",
    "\n",
    "    return max_medie.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d22b2daf9332c",
   "metadata": {},
   "source": [
    "### Alleno Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61be4943ea4b52a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.582975Z",
     "start_time": "2025-04-28T17:18:24.577535Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, y_train, random_state):\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948cce5c376ef3",
   "metadata": {},
   "source": [
    "### Divido i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52788d4b567a595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:18:24.613655Z",
     "start_time": "2025-04-28T17:18:24.602204Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(position, df_data, weight_list=None, varianza = False, lista_percentuali = None):\n",
    "    global df_f1_score\n",
    "    global f1_s_max_dict\n",
    "    global baseCalcolata\n",
    "    all_sensors = len(position) > 1\n",
    "    labels = df_data['Activity'].unique()\n",
    "    \n",
    "    if weight_list is None:\n",
    "        weight_list = [5,10,25]\n",
    "        #weight_list = [5, 10, 25, 50, 75, 100, 250, 500, 750, 1000, 1500, 2000, 2500, 5000, 7500, 10000]\n",
    "    if lista_percentuali is None:\n",
    "        #lista_percentuali = [i / 100 for i in range(0, 101, 5)]\n",
    "        lista_percentuali = [1]\n",
    "        \n",
    "    for rand_state in RANDOM_STATE_LIST:\n",
    "        for peso in weight_list:\n",
    "            for percentuale_nuovo_train in lista_percentuali:\n",
    "                print(\",\".join(position)+\" stato \"+str(rand_state)+\" \"+str(peso)+\"w \"+str(int(percentuale_nuovo_train*100))+\"%\")\n",
    "                for k in df_data['Userid'].unique():\n",
    "                    X_train, X_test, y_train, y_test, num_dati_spostati = \\\n",
    "                        get_train_test_data(df_data[df_data['position'].isin(position)], user=k, random_state = rand_state, percentage=float(percentuale_nuovo_train), weight = float(peso), varianza = varianza)\n",
    "\n",
    "                    start = time.perf_counter()\n",
    "                    y_pred = train_model(X_train, X_test, y_train,random_state = rand_state)\n",
    "                    end = time.perf_counter()\n",
    "                    durata = end - start\n",
    "                    class_report = classification_report(y_test, y_pred, output_dict=True,zero_division=0)\n",
    "                    \n",
    "                    for label_idx, label in zip(y_train.unique(), labels):\n",
    "                        df = pd.DataFrame()\n",
    "                        df['label'] = [label]\n",
    "                        df['timeUsed'] = [num_dati_spostati * ROW_TIME]\n",
    "                        df['percentage'] = [int(percentuale_nuovo_train*100)]\n",
    "                        df['weight'] = [float(peso)]\n",
    "                        df['time'] = [round(durata, 2)]\n",
    "                        df['randomState'] = [rand_state]\n",
    "                        df['position'] = ['all sensors'] if all_sensors else position\n",
    "\n",
    "                        key_formats = [str(label_idx), str(float(label_idx)), str(int(label_idx))]\n",
    "                        for key in key_formats:\n",
    "                            try:\n",
    "                                df['f1-score'] = [class_report[key]['f1-score']]\n",
    "                                df['precision'] = [class_report[key]['precision']]\n",
    "                                df['recall'] = [class_report[key]['recall']]\n",
    "                                break\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "\n",
    "                        df_f1_score = pd.concat([df_f1_score, df], axis=0).reset_index(drop=True)\n",
    "\n",
    "                if not baseCalcolata:\n",
    "                    continue\n",
    "                if all_sensors or position[0] in df_data['position'].unique():\n",
    "                    df_appena_calcolato = df_f1_score[df_f1_score['weight'] == peso]\n",
    "                    df_appena_calcolato = df_appena_calcolato[df_appena_calcolato['percentage'] == int(percentuale_nuovo_train*100)]\n",
    "\n",
    "                    if all_sensors:\n",
    "                        pos_key = 'all sensors'\n",
    "                        f1_s_max = f1_s_max_dict['all sensors']\n",
    "                    else:\n",
    "                        pos_key = position[0]\n",
    "                        f1_s_max = f1_s_max_dict[position[0]] \n",
    "\n",
    "                    f1_s_mifermo = prendiMax(df_appena_calcolato, pos_key, rand_state)\n",
    "                    if f1_s_mifermo[rand_state] >= f1_s_max[rand_state]:\n",
    "                        print(f\"  stop a {int(percentuale_nuovo_train * 100)}%({num_dati_spostati * ROW_TIME}) per peso {peso} \"\n",
    "                                f\"in quanto {f1_s_mifermo[rand_state]} Ã¨ maggiore del max a peso 1 ({f1_s_max[rand_state]})\")\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36f0b922f7056",
   "metadata": {},
   "source": [
    "### Caso Base\n",
    "Peso 1, fa da ottimizzatore per i veri modelli con tutti i vari pesi facendoli fermare quando superano il massimo di questo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dc366ad20fbe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:48:11.254152Z",
     "start_time": "2025-04-28T17:18:24.665728Z"
    }
   },
   "outputs": [],
   "source": [
    "baseCalcolata = False\n",
    "\n",
    "df_f1_score = pd.DataFrame()\n",
    "f1_s_max_dict = {}\n",
    "\n",
    "mypath = os.getcwd() + '/data/data_total/'\n",
    "file_pattern = NOMI_FILE['baseline']+'.csv'\n",
    "esiste_base = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and \n",
    "               re.compile(file_pattern).match(f))]\n",
    "\n",
    "if esiste_base:\n",
    "    df_f1_score = pd.read_csv(mypath + NOMI_FILE['baseline']+'.csv')\n",
    "else:\n",
    "    print(\"Position: all sensors\")\n",
    "    position = [pos for pos in df_data['position'].unique()]\n",
    "    k_fold_cross_validation(position, df_data, weight_list=[1])\n",
    "\n",
    "    for position in df_data['position'].unique():\n",
    "       print(\"Position: \", position)\n",
    "       k_fold_cross_validation([position], df_data, weight_list=[1])\n",
    "    if SAVE:   \n",
    "        df_f1_score.to_csv(mypath + NOMI_FILE['baseline']+'.csv')\n",
    "\n",
    "for position in df_data['position'].unique():\n",
    "   f1_s_max_dict[position] = prendiMax(df_f1_score, position, RANDOM_STATE_LIST)\n",
    "   print(\"f1-score \", position, \":\", f1_s_max_dict[position])\n",
    "\n",
    "f1_s_max_dict['all sensors'] = prendiMax(df_f1_score, 'all sensors', RANDOM_STATE_LIST)\n",
    "print(\"f1-score all sensors:\", f1_s_max_dict['all sensors'])\n",
    "\n",
    "\n",
    "pesoBaseData = df_f1_score.copy()\n",
    "\n",
    "baseCalcolata = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98dc28",
   "metadata": {},
   "source": [
    "### Calcolo modello base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521b5a040138e4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:03:47.576408Z",
     "start_time": "2025-04-28T17:48:11.436135Z"
    }
   },
   "outputs": [],
   "source": [
    "mypath = os.getcwd() + '/data/data_total/'\n",
    "file_pattern = NOMI_FILE['modello_base']+'.csv'\n",
    "esiste_base = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and \n",
    "               re.compile(file_pattern).match(f))]\n",
    "\n",
    "if not esiste_base:\n",
    "    df_f1_score = pd.DataFrame()\n",
    "\n",
    "    for position in df_data['position'].unique():\n",
    "        print(\"Position: \", position)\n",
    "        k_fold_cross_validation([position], df_data)\n",
    "\n",
    "    print(\"Position: all sensors\")\n",
    "    position = [pos for pos in df_data['position'].unique()]\n",
    "    k_fold_cross_validation(position, df_data)\n",
    "\n",
    "    if SAVE:   \n",
    "        baseData = df_f1_score.copy()\n",
    "        baseData = pd.concat([baseData, pesoBaseData]).reset_index(drop=True)\n",
    "        baseData.to_csv(mypath + NOMI_FILE['modello_base']+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041785e8693b551",
   "metadata": {},
   "source": [
    "### Calcolo modello con varianza\n",
    "Varianza definita come il moltiplicare ogni riga ripetuta per un valore compreso tra 0.99 e 1.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b187c8c5b769b68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:20:15.736753Z",
     "start_time": "2025-04-28T19:03:47.782481Z"
    }
   },
   "outputs": [],
   "source": [
    "mypath = os.getcwd() + '/data/data_total/'\n",
    "file_pattern = NOMI_FILE['modello_varianza']+'.csv'\n",
    "esiste_base = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and \n",
    "               re.compile(file_pattern).match(f))]\n",
    "\n",
    "if not esiste_base:\n",
    "\n",
    "    df_f1_score = pd.DataFrame()\n",
    "\n",
    "    for position in df_data['position'].unique():\n",
    "        print(\"Position: \", position)\n",
    "        k_fold_cross_validation([position], df_data, varianza = True)\n",
    "\n",
    "    print(\"Position: all sensors\")\n",
    "    position = [pos for pos in df_data['position'].unique()]\n",
    "    k_fold_cross_validation(position, df_data, varianza = True)\n",
    "\n",
    "    if SAVE:   \n",
    "        varianzaData = df_f1_score.copy()\n",
    "        varianzaData = pd.concat([varianzaData, pesoBaseData]).reset_index(drop=True)\n",
    "        varianzaData.to_csv(mypath + NOMI_FILE['modello_varianza']+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

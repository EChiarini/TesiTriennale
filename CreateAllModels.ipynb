{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f958707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from joblib import Parallel, delayed\n",
    "from genericpath import isfile\n",
    "from ntpath import join\n",
    "from os import listdir\n",
    "import glob\n",
    "import cProfile, pstats # per vedere quanti ci metto\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8fa23d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_each_sensor_optimized(df_data_input, positions_list):\n",
    "    if not positions_list or df_data_input.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    grouping_keys = ['Userid', 'label']\n",
    "    # Colonne da escludere dalla ridenominazione delle feature\n",
    "    meta_cols_to_exclude_from_rename = ['label', 'position', 'Userid','Activity']\n",
    "\n",
    "    all_processed_group_dfs = []\n",
    "    # Itera su ogni gruppo (Userid, label)\n",
    "    for (uid, lbl), group_df in df_data_input.groupby(grouping_keys, observed=False):\n",
    "        processed_dfs_for_group_concat = []\n",
    "        labels_columns_for_mask_group = []\n",
    "        \n",
    "        min_len_for_group_alignment = float('inf')\n",
    "        temp_pos_dfs_for_group = {}\n",
    "\n",
    "        #trova la lunghezza minima tra le posizioni\n",
    "        actual_positions_in_group = [p for p in positions_list if p in group_df['position'].unique()]\n",
    "        if len(actual_positions_in_group) != len(positions_list):\n",
    "            #non vuoi entrare qui\n",
    "            continue \n",
    "\n",
    "        for position_val in actual_positions_in_group: # Usa actual_positions_in_group\n",
    "            df_pos_filtered_group = group_df[group_df['position'] == position_val]\n",
    "            temp_pos_dfs_for_group[position_val] = df_pos_filtered_group\n",
    "            if len(df_pos_filtered_group) < min_len_for_group_alignment:\n",
    "                min_len_for_group_alignment = len(df_pos_filtered_group)\n",
    "        \n",
    "        if min_len_for_group_alignment == 0 or min_len_for_group_alignment == float('inf'):\n",
    "            #non vuoi entrare qui\n",
    "            continue\n",
    "\n",
    "        for position_val in actual_positions_in_group:\n",
    "            df_pos_segment_group = temp_pos_dfs_for_group[position_val].head(min_len_for_group_alignment).reset_index(drop=True)\n",
    "\n",
    "            cols_to_rename_group = [col for col in df_pos_segment_group.columns if col not in meta_cols_to_exclude_from_rename]\n",
    "            rename_dict_group = {col: f\"{col}_{position_val}\" for col in cols_to_rename_group}\n",
    "            \n",
    "            df_features_renamed_group = df_pos_segment_group.rename(columns=rename_dict_group)[list(rename_dict_group.values())]\n",
    "\n",
    "            label_col_name_for_mask_group = f'label_{position_val}' # Usato solo per il check mask\n",
    "            df_label_for_mask_group = df_pos_segment_group[['label']].rename(columns={'label': label_col_name_for_mask_group})\n",
    "            labels_columns_for_mask_group.append(label_col_name_for_mask_group)\n",
    "            \n",
    "            current_df_part_group = pd.concat([df_features_renamed_group, df_label_for_mask_group], axis=1)\n",
    "            processed_dfs_for_group_concat.append(current_df_part_group)\n",
    "\n",
    "        if not processed_dfs_for_group_concat:\n",
    "            continue\n",
    "        \n",
    "        df_group_final_wide = pd.concat(processed_dfs_for_group_concat, axis=1)\n",
    "\n",
    "        # Applica la maschera per le label\n",
    "        existing_labels_cols_group = [col for col in labels_columns_for_mask_group if col in df_group_final_wide.columns]\n",
    "        if not existing_labels_cols_group: continue # Non dovrebbe succedere\n",
    "\n",
    "        if len(existing_labels_cols_group) > 1:\n",
    "            label_data_group = df_group_final_wide[existing_labels_cols_group]\n",
    "            mask_group = label_data_group.nunique(axis=1, dropna=True) <= 1\n",
    "            df_group_filtered_wide = df_group_final_wide[mask_group]\n",
    "        else: # Solo una colonna label, nessun filtro mask necessario\n",
    "            df_group_filtered_wide = df_group_final_wide\n",
    "        \n",
    "        if df_group_filtered_wide.empty: continue\n",
    "\n",
    "        # Rinomina la prima colonna label in 'label' e droppa le altre (ridondanti)\n",
    "        df_group_filtered_wide = df_group_filtered_wide.rename(columns={existing_labels_cols_group[0]: 'label'})\n",
    "        if len(existing_labels_cols_group) > 1:\n",
    "            cols_to_drop_labels_group = [col for col in existing_labels_cols_group[1:] if col in df_group_filtered_wide.columns]\n",
    "            if cols_to_drop_labels_group:\n",
    "                 df_group_filtered_wide = df_group_filtered_wide.drop(columns=cols_to_drop_labels_group)\n",
    "\n",
    "        df_group_filtered_wide['Userid'] = uid\n",
    "        all_processed_group_dfs.append(df_group_filtered_wide)\n",
    "\n",
    "    if not all_processed_group_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    final_combined_df = pd.concat(all_processed_group_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f2c09c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicaRighePesi(df_moved, weight): #solo per varianza\n",
    "    if df_moved.empty: return df_moved\n",
    "    df_moved['is_original'] = True\n",
    "    repeated_part = df_moved.loc[np.repeat(df_moved.index, int(weight) - 1)].copy()\n",
    "    repeated_part['is_original'] = False\n",
    "    df_moved = pd.concat([df_moved, repeated_part], ignore_index=True)\n",
    "    feature_cols = df_moved.columns.difference(['label', 'is_original', 'Userid', 'Timestamp', 'Activity', 'position'])\n",
    "    feature_cols = df_moved[feature_cols].select_dtypes(include=[np.number]).columns\n",
    "    if not feature_cols.empty:\n",
    "        df_moved.loc[~df_moved['is_original'], feature_cols] *= np.random.uniform(0.99, 1.01, size=df_moved.loc[~df_moved['is_original'], feature_cols].shape)\n",
    "    df_moved = df_moved.drop('is_original', axis=1, errors='ignore')\n",
    "    return df_moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f684dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df_data_input, user=None, random_state=42, weight = -1, varianza = False, \n",
    "                        features_list=None, all_positions_list=None, row_to_move = 0):\n",
    "\n",
    "    df_train = df_data_input[df_data_input['Userid'] != user].reset_index(drop=True)\n",
    "    df_test = df_data_input[df_data_input['Userid'] == user].reset_index(drop=True)\n",
    "\n",
    "    if len(all_positions_list) > 1:\n",
    "        df_train = get_features_for_each_sensor_optimized(df_train[features_list + ['position', 'label', 'Userid','Activity']], all_positions_list)\n",
    "        df_test  = get_features_for_each_sensor_optimized(df_test[features_list + ['position', 'label', 'Userid','Activity']], all_positions_list)\n",
    "    df_sampling_pool, df_testFISSO = train_test_split(df_test, test_size=0.2, random_state=random_state,stratify=df_test['label'])\n",
    "\n",
    "    #sposto le righe\n",
    "    num_to_move = 0\n",
    "    moved_indices = []\n",
    "    if row_to_move > 0:\n",
    "        for label_value in df_sampling_pool['label'].unique():\n",
    "            df_test_label = df_sampling_pool[df_sampling_pool['label'] == label_value]\n",
    "            indices_to_move = df_test_label.sample(n=row_to_move, random_state=random_state).index.tolist()\n",
    "            moved_indices.extend(indices_to_move)\n",
    "\n",
    "    total_rows_moved = len(moved_indices)\n",
    "\n",
    "    if moved_indices:\n",
    "        df_moved = df_sampling_pool.loc[moved_indices].copy()\n",
    "        if varianza and weight > 1:\n",
    "            df_moved = duplicaRighePesi(df_moved, weight)\n",
    "        df_train = pd.concat([df_train, df_moved], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    if len(all_positions_list) > 1:\n",
    "        X_train = df_train.drop(columns=['label','Userid'])\n",
    "        X_test = df_testFISSO.drop(columns=['label','Userid'])\n",
    "    else:\n",
    "        X_train = df_train[features_list]\n",
    "        X_test = df_testFISSO[features_list]\n",
    "    y_train = df_train['label']\n",
    "    y_test  = df_testFISSO['label']\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, total_rows_moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2dcb406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, y_train, random_state, dove_peso):\n",
    "    class_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    final_sample_weights = dove_peso * class_weights\n",
    "\n",
    "    xgb = XGBClassifier( n_estimators=150, random_state=random_state, n_jobs=-1)\n",
    "    xgb.fit(X_train, y_train, sample_weight = final_sample_weights)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_random_state(rand_state, current_position_list, df_data_arg, weight_list_arg,\n",
    "                                lista_minuti_arg, varianza_arg, ROW_TIME_arg, OVERLAP_arg):\n",
    "    print(f\"Avvio random_state: {rand_state}, posizione: {','.join(current_position_list)}\")\n",
    "    single_state_results = []\n",
    "    all_sensors_flag = len(current_position_list) > 1\n",
    "    current_pos_key_str = 'all sensors' if all_sensors_flag else current_position_list[0]\n",
    "\n",
    "    all_original_features = [item for item in df_data_arg.columns if\n",
    "                             item not in ['Timestamp', 'Userid', 'UserAge', 'UserSex', 'UserHeight', 'UserWeight', 'Activity',\n",
    "                                          'position', 'label', 'MagnxEnergy', 'MagnyEnergy', 'MagnzEnergy', 'MagnMagnitude',\n",
    "                                          'MagnMagnitudeMean', 'MagnMagnitudeMin', 'MagnMagnitudeMax', 'MagnMagnitudeStd',\n",
    "                                          'MagnMagnitudeEnergy']]\n",
    "    all_original_features = [item for item in all_original_features if not re.match(r'.*MagnMagnitude.*', item)]\n",
    "    selected_original_features = [item for item in all_original_features if re.match(r'.*Magnitude.*', item)]\n",
    "\n",
    "    df_position = df_data_arg[df_data_arg['position'].isin(current_position_list)]\n",
    "    labels = df_data_arg['Activity'].unique()\n",
    "\n",
    "    if weight_list_arg is None:\n",
    "        crowd_results = []\n",
    "        for k_user in df_position['Userid'].unique():\n",
    "            X_train, X_test, y_train, y_test, num_dati_spostati = \\\n",
    "                            get_train_test_data(\n",
    "                                df_position,\n",
    "                                user=k_user, random_state=rand_state,\n",
    "                                features_list=selected_original_features,\n",
    "                                all_positions_list=current_position_list\n",
    "                            )\n",
    "\n",
    "            sample_weight = [1] * len(X_train)\n",
    "            y_pred = train_model(X_train, X_test, y_train, random_state=rand_state,dove_peso=sample_weight)\n",
    "\n",
    "            if len(y_test) == 0 or len(y_pred) == 0:\n",
    "                print(\"errore che non vuoi avere\") #crasha tutto\n",
    "                macro_f1 = np.nan; macro_precision = np.nan; macro_recall = np.nan\n",
    "            else:\n",
    "                class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "                macro_f1 = class_report.get('macro avg', {}).get('f1-score', np.nan)\n",
    "                macro_precision = class_report.get('macro avg', {}).get('precision', np.nan)\n",
    "                macro_recall = class_report.get('macro avg', {}).get('recall', np.nan)\n",
    "\n",
    "            current_metrics = {\n",
    "                'k_user': k_user,\n",
    "                'randomState': rand_state,\n",
    "                'position': current_pos_key_str,\n",
    "                'f1-score': macro_f1,\n",
    "                'precision': macro_precision, \n",
    "                'recall': macro_recall\n",
    "            }\n",
    "            crowd_results.append(current_metrics)\n",
    "\n",
    "        return crowd_results\n",
    "    else:\n",
    "        minimo_gruppo = df_position.groupby(['Userid', 'label', 'position']).size().min()\n",
    "        minimo_disponibile = math.floor(minimo_gruppo * 0.8)\n",
    "        for peso_arg in weight_list_arg:\n",
    "            for secondi_nuovo_train_arg in lista_minuti_arg:\n",
    "                target_n_total = 1 + (secondi_nuovo_train_arg - ROW_TIME_arg) / (ROW_TIME_arg * (1 - OVERLAP_arg))\n",
    "                num_ideale_per_classe = max(1, target_n_total)\n",
    "                final_num_to_move = int(min(num_ideale_per_classe, minimo_disponibile))\n",
    "                tempoEffettivo = ROW_TIME_arg * (1 + (final_num_to_move - 1) * (1 - OVERLAP_arg))\n",
    "                \n",
    "                results_this_minute_all_k = []\n",
    "                for k_user in df_position['Userid'].unique():\n",
    "                    X_train, X_test, y_train, y_test, num_dati_spostati = \\\n",
    "                        get_train_test_data(\n",
    "                            df_position,\n",
    "                            user=k_user, random_state=rand_state,\n",
    "                            weight=int(peso_arg), varianza=varianza_arg,\n",
    "                            features_list=selected_original_features,\n",
    "                            all_positions_list=current_position_list,\n",
    "                            row_to_move = final_num_to_move\n",
    "                        )\n",
    "                    if X_train.empty or X_test.empty or y_train.empty or y_test.empty:\n",
    "                        print(\"errore che non vuoi avere\")\n",
    "                        continue\n",
    "\n",
    "                    if not varianza_arg and peso_arg > 1:\n",
    "                        len_train = len(X_train) - num_dati_spostati\n",
    "                        sample_weight = [1] * len_train + [peso_arg] * num_dati_spostati\n",
    "                    else:\n",
    "                        sample_weight = [1] * len(X_train)\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    y_pred = train_model(X_train, X_test, y_train, random_state=rand_state,dove_peso=sample_weight)\n",
    "                    end = time.perf_counter()\n",
    "                    durata = end - start\n",
    "\n",
    "\n",
    "\n",
    "                    if len(y_test) == 0 or len(y_pred) == 0:\n",
    "                        print(\"errore che non vuoi avere\") #crasha tutto\n",
    "                        macro_f1 = np.nan; macro_precision = np.nan; macro_recall = np.nan\n",
    "                    else:\n",
    "                        class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "                        macro_f1 = class_report.get('macro avg', {}).get('f1-score', np.nan)\n",
    "                        macro_precision = class_report.get('macro avg', {}).get('precision', np.nan)\n",
    "                        macro_recall = class_report.get('macro avg', {}).get('recall', np.nan)\n",
    "\n",
    "                    current_metrics = {\n",
    "                        'k_user': k_user,\n",
    "                        'timeUsed': int(tempoEffettivo),\n",
    "                        'weight': int(peso_arg),\n",
    "                        'time': round(durata, 2),\n",
    "                        'randomState': rand_state,\n",
    "                        'position': current_pos_key_str,\n",
    "                        'f1-score': macro_f1,\n",
    "                        'precision': macro_precision, \n",
    "                        'recall': macro_recall\n",
    "                    }\n",
    "\n",
    "                    for label_idx, label in zip(y_train.unique(), labels):\n",
    "                        current_metrics[f'f1_{label}'] =  class_report[str(label_idx)]['f1-score']\n",
    "                        current_metrics[f'precision_{label}'] = class_report[str(label_idx)]['precision']\n",
    "                        current_metrics[f'recall_{label}'] = class_report[str(label_idx)]['recall']\n",
    "\n",
    "                    results_this_minute_all_k.append(current_metrics)\n",
    "\n",
    "                single_state_results.extend(results_this_minute_all_k)\n",
    "\n",
    "        return single_state_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_parallel(\n",
    "    position_list_arg,\n",
    "    df_data_arg,\n",
    "    weight_list_param=None,\n",
    "    varianza_param=False,\n",
    "    random_state_list_global=None,\n",
    "    ROW_TIME_global=None,\n",
    "    OVERLAP_global=None,\n",
    "    minute_list_global=None\n",
    "    ):\n",
    "\n",
    "    ### DA USARE SOLO PER TESTING\n",
    "    # ci vedo i print + le prestazioni commentate sotto\n",
    "\n",
    "    # x = pd.DataFrame()\n",
    "    # for rs in random_state_list_global:\n",
    "    #     results_test_run = process_single_random_state(    \n",
    "    #             rs,\n",
    "    #             position_list_arg,\n",
    "    #             df_data_arg,\n",
    "    #             weight_list_param,\n",
    "    #             minute_list_global,\n",
    "    #             varianza_param,\n",
    "    #             ROW_TIME_global,\n",
    "    #             OVERLAP_global\n",
    "    #     )\n",
    "    #     y = pd.DataFrame(results_test_run)\n",
    "    #     x = pd.concat([x, y], axis=1)\n",
    "    # return x\n",
    "\n",
    "    num_cores = os.cpu_count()\n",
    "\n",
    "    parallel_outputs = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_single_random_state)(\n",
    "            rs,\n",
    "            position_list_arg,\n",
    "            df_data_arg,\n",
    "            weight_list_param,\n",
    "            minute_list_global,\n",
    "            varianza_param,\n",
    "            ROW_TIME_global,\n",
    "            OVERLAP_global\n",
    "        ) for rs in random_state_list_global\n",
    "    )\n",
    "    all_results_list = [item for sublist in parallel_outputs for item in sublist]\n",
    "        \n",
    "    return pd.DataFrame(all_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39d318c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati pre-combinati\n"
     ]
    }
   ],
   "source": [
    "def load_variables_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        code = f.read()\n",
    "        local_vars = {}\n",
    "        exec(code, {}, local_vars)\n",
    "        return local_vars\n",
    "vars = load_variables_from_file('Configurazioni.txt')\n",
    "\n",
    "RANDOM_STATE_LIST = vars['RANDOM_STATE_LIST']\n",
    "SECONDS_LIST = vars['SECONDS_LIST']\n",
    "WEIGHT_LIST = vars['WEIGHT_LIST']\n",
    "DATASET = vars['DATASET']\n",
    "\n",
    "OVERLAP = 0.5\n",
    "ROW_TIME = 4\n",
    "SAVE = True\n",
    "COMBINED_POSITIONS = False\n",
    "\n",
    "if DATASET == 'MultiPositionWearable':\n",
    "  NOMI_FILE = {\n",
    "    'baseline_crowd': 'MultiPositionWearable_baseline_crowd',\n",
    "    'modello_base_crowd': 'MultiPositionWearable_base_crowd',\n",
    "    'modello_varianza_crowd': 'MultiPositionWearable_varianza_crowd',\n",
    "    'cartella_dati': 'MultiPositionWearable_processed_data'\n",
    "  }\n",
    "elif DATASET == 'selfBACK':\n",
    "  NOMI_FILE = {\n",
    "    'baseline_crowd': 'selfBACK_baseline_crowd',\n",
    "    'modello_base_crowd': 'selfBACK_base_crowd',\n",
    "    'modello_varianza_crowd': 'selfBACK_varianza_crowd',\n",
    "    'cartella_dati': 'selfBACK_processed_data'\n",
    "  }\n",
    "elif DATASET == 'SDALLE':\n",
    "  NOMI_FILE = {\n",
    "    'baseline_crowd': 'SDALLE_baseline_crowd',\n",
    "    'modello_base_crowd': 'SDALLE_base_crowd',\n",
    "    'modello_varianza_crowd': 'SDALLE_varianza_crowd',\n",
    "    'cartella_dati': 'SDALLE_processed_data'\n",
    "  }\n",
    "else:\n",
    "    raise ValueError(\"DATASET non valido. Scegliere tra 'MultiPositionWearable', 'selfBACK' o 'SDALLE'\")\n",
    "\n",
    "mypath_carica = os.getcwd() + '/data/' + NOMI_FILE['cartella_dati'] + '/'\n",
    "file_pattern = os.path.join(mypath_carica, 'grouped_data*.csv')\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "# Filtra i file che non contengono '_combined'\n",
    "file_list = [f for f in all_files if '_combined' not in os.path.basename(f)]\n",
    "\n",
    "df_data = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_temp = pd.read_csv(file, header=0)\n",
    "    if df_temp.columns[0].lower() in ['unnamed: 0', 'unnamed: 0.1']:\n",
    "      df_temp = df_temp.iloc[:, 1:]\n",
    "    df_data = pd.concat([df_data, df_temp], ignore_index=True)\n",
    "\n",
    "def set_labels(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['Activity'])\n",
    "    return df, label_encoder.classes_\n",
    "df_data, labels_activity_names = set_labels(df_data)\n",
    "def strip_Spaces(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "df_data = strip_Spaces(df_data)\n",
    "\n",
    "\n",
    "\n",
    "if len(all_files) - len(file_list) > 0:\n",
    "  COMBINED_POSITIONS = True\n",
    "  print(\"Dati pre-combinati\")\n",
    "  file_list = [f for f in all_files if '_combined' in os.path.basename(f)]\n",
    "  df_data_combined = pd.DataFrame()\n",
    "  for file in file_list:\n",
    "      df_temp = pd.read_csv(file, header=0)\n",
    "      if df_temp.columns[0].lower() in ['unnamed: 0', 'unnamed: 0.1']:\n",
    "        df_temp = df_temp.iloc[:, 1:]\n",
    "      df_data_combined = pd.concat([df_data_combined, df_temp], ignore_index=True)\n",
    "  df_data_combined, labels_activity_names_combined = set_labels(df_data_combined)\n",
    "  df_data_combined = strip_Spaces(df_data_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a12ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento baseline da: c:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale/data/data_total/SDALLE_baseline_crowd.csv\n"
     ]
    }
   ],
   "source": [
    "mypath_data_total = os.getcwd() + '/data/data_total/'\n",
    "os.makedirs(mypath_data_total, exist_ok=True)\n",
    "path_baseline_csv = mypath_data_total + NOMI_FILE['baseline_crowd'] + '.csv'\n",
    "\n",
    "list_df_baseline_parts = []\n",
    "\n",
    "if os.path.exists(path_baseline_csv) and SAVE:\n",
    "    print(f\"Caricamento baseline da: {path_baseline_csv}\")\n",
    "    pesoBaseData = pd.read_csv(path_baseline_csv)\n",
    "else:\n",
    "    # DA USARE SOLO PER TESTING\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()    \n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "    \n",
    "    if len(all_available_positions_from_df) > 1:\n",
    "        print(\"Calcolo baseline per: all sensors\")\n",
    "\n",
    "        if COMBINED_POSITIONS:\n",
    "            position_list = df_data_combined['position'].unique().tolist()\n",
    "            data = df_data_combined\n",
    "        else:\n",
    "            position_list = all_available_positions_from_df\n",
    "            data = df_data\n",
    "\n",
    "        df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "            position_list, data,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "\n",
    "        list_df_baseline_parts.append(df_all_sensors_model)\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Calcolo baseline per: {pos_single}\")\n",
    "        df_pos_single_baseline = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data, \n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "        list_df_baseline_parts.append(df_pos_single_baseline)\n",
    "    \n",
    "    # DA USARE SOLO PER TESTING\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "    # stats.print_stats(30)\n",
    "    \n",
    "    if list_df_baseline_parts:\n",
    "        pesoBaseData = pd.concat(list_df_baseline_parts, ignore_index=True)\n",
    "        if SAVE:\n",
    "            pesoBaseData.to_csv(path_baseline_csv, index=False)\n",
    "    else:\n",
    "        pesoBaseData = pd.DataFrame() # Vuoto se nessun risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b3731d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo base per: all sensors\n",
      "Avvio random_state: 1, posizione: all position\n",
      "-0.5 vs 9\n",
      " e alla fine segno 2.0\n",
      "4.0 vs 9\n",
      " e alla fine segno 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m         position_list = all_available_positions_from_df\n\u001b[32m     19\u001b[39m         data = df_data\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     df_all_sensors_model = \u001b[43mk_fold_cross_validation_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_list_param\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_LIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state_list_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRANDOM_STATE_LIST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mROW_TIME_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mROW_TIME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mOVERLAP_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOVERLAP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminute_list_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSECONDS_LIST\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     list_df_model_base_parts.append(df_all_sensors_model)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pos_single \u001b[38;5;129;01min\u001b[39;00m all_available_positions_from_df:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mk_fold_cross_validation_parallel\u001b[39m\u001b[34m(position_list_arg, df_data_arg, weight_list_param, varianza_param, random_state_list_global, ROW_TIME_global, OVERLAP_global, minute_list_global)\u001b[39m\n\u001b[32m     15\u001b[39m x = pd.DataFrame()\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m random_state_list_global:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     results_test_run = \u001b[43mprocess_single_random_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m    \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposition_list_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdf_data_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mweight_list_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43mminute_list_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvarianza_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43mROW_TIME_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mOVERLAP_global\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     y = pd.DataFrame(results_test_run)\n\u001b[32m     28\u001b[39m     x = pd.concat([x, y], axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mprocess_single_random_state\u001b[39m\u001b[34m(rand_state, current_position_list, df_data_arg, weight_list_arg, lista_minuti_arg, varianza_arg, ROW_TIME_arg, OVERLAP_arg)\u001b[39m\n\u001b[32m     87\u001b[39m     sample_weight = [\u001b[32m1\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(X_train)\n\u001b[32m     89\u001b[39m start = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m y_pred = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrand_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdove_peso\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m end = time.perf_counter()\n\u001b[32m     92\u001b[39m durata = end - start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(X_train, X_test, y_train, random_state, dove_peso)\u001b[39m\n\u001b[32m      3\u001b[39m final_sample_weights = dove_peso * class_weights\n\u001b[32m      5\u001b[39m xgb = XGBClassifier( n_estimators=\u001b[32m150\u001b[39m, random_state=random_state, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_sample_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m y_pred = xgb.predict(X_test)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1660\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1661\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1662\u001b[39m )\n\u001b[32m   1663\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1664\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1665\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1679\u001b[39m     feature_types=\u001b[38;5;28mself\u001b[39m.feature_types,\n\u001b[32m   1680\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1697\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\core.py:2243\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtrain, DMatrix):\n\u001b[32m   2242\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid training matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dtrain).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2243\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_dmatrix_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m   2247\u001b[39m         _LIB.XGBoosterUpdateOneIter(\n\u001b[32m   2248\u001b[39m             \u001b[38;5;28mself\u001b[39m.handle, ctypes.c_int(iteration), dtrain.handle\n\u001b[32m   2249\u001b[39m         )\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\core.py:3203\u001b[39m, in \u001b[36mBooster._assign_dmatrix_features\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   3200\u001b[39m fn = data.feature_names\n\u001b[32m   3201\u001b[39m ft = data.feature_types\n\u001b[32m-> \u001b[39m\u001b[32m3203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_names\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3204\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_names = fn\n\u001b[32m   3205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\core.py:2193\u001b[39m, in \u001b[36mBooster.feature_names\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2187\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m   2188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeature_names\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[FeatureNames]:\n\u001b[32m   2189\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Feature names for this booster.  Can be directly set by input data or by\u001b[39;00m\n\u001b[32m   2190\u001b[39m \u001b[33;03m    assignment.\u001b[39;00m\n\u001b[32m   2191\u001b[39m \n\u001b[32m   2192\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_feature_info\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale\\.venv\\Lib\\site-packages\\xgboost\\core.py:2143\u001b[39m, in \u001b[36mBooster._get_feature_info\u001b[39m\u001b[34m(self, field)\u001b[39m\n\u001b[32m   2140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhandle\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2142\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterGetStrFeatureInfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43msarr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2148\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2149\u001b[39m )\n\u001b[32m   2150\u001b[39m feature_info = from_cstr_to_pystr(sarr, length)\n\u001b[32m   2151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feature_info \u001b[38;5;28;01mif\u001b[39;00m feature_info \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "path_model_base_csv = mypath_data_total + NOMI_FILE['modello_base_crowd'] + '.csv'\n",
    "if os.path.exists(path_model_base_csv) and SAVE:\n",
    "    print(f\"Modello base già calcolato, caricamento da: {path_model_base_csv}\")\n",
    "    baseData = pd.read_csv(path_model_base_csv)\n",
    "else:    \n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    list_df_model_base_parts = []\n",
    "\n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "    if len(all_available_positions_from_df) > 1:\n",
    "        print(\"Calcolo base per: all sensors\")\n",
    "\n",
    "        if COMBINED_POSITIONS:\n",
    "            position_list = df_data_combined['position'].unique().tolist()\n",
    "            data = df_data_combined\n",
    "        else:\n",
    "            position_list = all_available_positions_from_df\n",
    "            data = df_data\n",
    "\n",
    "        df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "            position_list, data,\n",
    "            weight_list_param=WEIGHT_LIST,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "\n",
    "        list_df_model_base_parts.append(df_all_sensors_model)\n",
    "\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Calcolo base per: {pos_single}\")\n",
    "        df_pos_single_model = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data,\n",
    "            weight_list_param=WEIGHT_LIST,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "\n",
    "        list_df_model_base_parts.append(df_pos_single_model)\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "    # stats.print_stats(30)\n",
    "    baseData = pd.concat(list_df_model_base_parts, ignore_index=True)\n",
    "\n",
    "    if SAVE:\n",
    "        baseData.to_csv(path_model_base_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello varianza già calcolato, caricamento da: c:\\Users\\emili\\Desktop\\Tirocinio\\Activity_Recognition\\TesiTriennale/data/data_total/SDALLE_varianza_crowd.csv\n"
     ]
    }
   ],
   "source": [
    "path_model_varianza_csv = mypath_data_total + NOMI_FILE['modello_varianza_crowd'] + '.csv'\n",
    "\n",
    "if os.path.exists(path_model_varianza_csv) and SAVE:\n",
    "    print(f\"Modello varianza già calcolato, caricamento da: {path_model_varianza_csv}\")\n",
    "    varianzaData = pd.read_csv(path_model_varianza_csv)\n",
    "else:\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    # print(\"Calcolo modello_varianza...\")\n",
    "    list_df_model_varianza_parts = []\n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "    if len(all_available_positions_from_df) > 1:\n",
    "        print(\"Processing varianza per: all sensors (combinato)\")\n",
    "        if COMBINED_POSITIONS:\n",
    "            position_list = df_data_combined['position'].unique().tolist()\n",
    "            data = df_data_combined\n",
    "        else:\n",
    "            position_list = all_available_positions_from_df\n",
    "            data = df_data\n",
    "\n",
    "        df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "            position_list, data,\n",
    "            weight_list_param=WEIGHT_LIST, varianza_param=True,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "        list_df_model_varianza_parts.append(df_all_sensors_model)\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Processing varianza per: {pos_single}\")\n",
    "        df_pos_single_varianza = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data,\n",
    "            weight_list_param=WEIGHT_LIST, varianza_param=True,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "        list_df_model_varianza_parts.append(df_pos_single_varianza)\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "    # stats.print_stats(30)\n",
    "    varianzaData = pd.concat(list_df_model_varianza_parts, ignore_index=True)\n",
    "\n",
    "    if SAVE:\n",
    "        varianzaData.to_csv(path_model_varianza_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

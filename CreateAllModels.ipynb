{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from joblib import Parallel, delayed\n",
    "from genericpath import isfile\n",
    "from ntpath import join\n",
    "from os import listdir\n",
    "import glob\n",
    "import cProfile, pstats # per vedere quanti ci metto\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa23d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_each_sensor_optimized(df_data_input, positions_list):\n",
    "    if not positions_list or df_data_input.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    grouping_keys = ['Userid', 'label']\n",
    "    # Colonne da escludere dalla ridenominazione delle feature\n",
    "    meta_cols_to_exclude_from_rename = ['label', 'position', 'Userid','Activity']\n",
    "\n",
    "    all_processed_group_dfs = []\n",
    "    # Itera su ogni gruppo (Userid, label)\n",
    "    for (uid, lbl), group_df in df_data_input.groupby(grouping_keys, observed=False):\n",
    "        processed_dfs_for_group_concat = []\n",
    "        labels_columns_for_mask_group = []\n",
    "        \n",
    "        min_len_for_group_alignment = float('inf')\n",
    "        temp_pos_dfs_for_group = {}\n",
    "\n",
    "        #trova la lunghezza minima tra le posizioni\n",
    "        actual_positions_in_group = [p for p in positions_list if p in group_df['position'].unique()]\n",
    "        if len(actual_positions_in_group) != len(positions_list):\n",
    "            #non vuoi entrare qui\n",
    "            continue \n",
    "\n",
    "        for position_val in actual_positions_in_group: # Usa actual_positions_in_group\n",
    "            df_pos_filtered_group = group_df[group_df['position'] == position_val]\n",
    "            temp_pos_dfs_for_group[position_val] = df_pos_filtered_group\n",
    "            if len(df_pos_filtered_group) < min_len_for_group_alignment:\n",
    "                min_len_for_group_alignment = len(df_pos_filtered_group)\n",
    "        \n",
    "        if min_len_for_group_alignment == 0 or min_len_for_group_alignment == float('inf'):\n",
    "            #non vuoi entrare qui\n",
    "            continue\n",
    "\n",
    "        #rocessa e prepara per la concatenazione orizzontale\n",
    "        for position_val in actual_positions_in_group:\n",
    "            df_pos_segment_group = temp_pos_dfs_for_group[position_val].head(min_len_for_group_alignment).reset_index(drop=True)\n",
    "\n",
    "            cols_to_rename_group = [col for col in df_pos_segment_group.columns if col not in meta_cols_to_exclude_from_rename]\n",
    "            rename_dict_group = {col: f\"{col}_{position_val}\" for col in cols_to_rename_group}\n",
    "            \n",
    "            df_features_renamed_group = df_pos_segment_group.rename(columns=rename_dict_group)[list(rename_dict_group.values())]\n",
    "\n",
    "            label_col_name_for_mask_group = f'label_{position_val}' # Usato solo per il check mask\n",
    "            df_label_for_mask_group = df_pos_segment_group[['label']].rename(columns={'label': label_col_name_for_mask_group})\n",
    "            labels_columns_for_mask_group.append(label_col_name_for_mask_group)\n",
    "            \n",
    "            current_df_part_group = pd.concat([df_features_renamed_group, df_label_for_mask_group], axis=1)\n",
    "            processed_dfs_for_group_concat.append(current_df_part_group)\n",
    "\n",
    "        if not processed_dfs_for_group_concat:\n",
    "            continue\n",
    "        \n",
    "        df_group_final_wide = pd.concat(processed_dfs_for_group_concat, axis=1)\n",
    "\n",
    "        # Applica la maschera per le label\n",
    "        existing_labels_cols_group = [col for col in labels_columns_for_mask_group if col in df_group_final_wide.columns]\n",
    "        if not existing_labels_cols_group: continue # Non dovrebbe succedere\n",
    "\n",
    "        if len(existing_labels_cols_group) > 1:\n",
    "            label_data_group = df_group_final_wide[existing_labels_cols_group]\n",
    "            mask_group = label_data_group.nunique(axis=1, dropna=True) <= 1\n",
    "            df_group_filtered_wide = df_group_final_wide[mask_group]\n",
    "        else: # Solo una colonna label, nessun filtro mask necessario\n",
    "            df_group_filtered_wide = df_group_final_wide\n",
    "        \n",
    "        if df_group_filtered_wide.empty: continue\n",
    "\n",
    "        # Rinomina la prima colonna label in 'label' e droppa le altre (ridondanti)\n",
    "        df_group_filtered_wide = df_group_filtered_wide.rename(columns={existing_labels_cols_group[0]: 'label'})\n",
    "        if len(existing_labels_cols_group) > 1:\n",
    "            cols_to_drop_labels_group = [col for col in existing_labels_cols_group[1:] if col in df_group_filtered_wide.columns]\n",
    "            if cols_to_drop_labels_group:\n",
    "                 df_group_filtered_wide = df_group_filtered_wide.drop(columns=cols_to_drop_labels_group)\n",
    "\n",
    "        df_group_filtered_wide['Userid'] = uid\n",
    "        all_processed_group_dfs.append(df_group_filtered_wide)\n",
    "\n",
    "    if not all_processed_group_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    final_combined_df = pd.concat(all_processed_group_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c09c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicaRighePesi(df_moved, weight): #solo per varianza\n",
    "    if df_moved.empty: return df_moved\n",
    "    df_moved['is_original'] = True\n",
    "    repeated_part = df_moved.loc[np.repeat(df_moved.index, int(weight) - 1)].copy()\n",
    "    repeated_part['is_original'] = False\n",
    "    df_moved = pd.concat([df_moved, repeated_part], ignore_index=True)\n",
    "    feature_cols = df_moved.columns.difference(['label', 'is_original', 'Userid', 'Timestamp', 'Activity', 'position'])\n",
    "    feature_cols = df_moved[feature_cols].select_dtypes(include=[np.number]).columns\n",
    "    if not feature_cols.empty:\n",
    "        df_moved.loc[~df_moved['is_original'], feature_cols] *= np.random.uniform(0.99, 1.01, size=df_moved.loc[~df_moved['is_original'], feature_cols].shape)\n",
    "    df_moved = df_moved.drop('is_original', axis=1, errors='ignore')\n",
    "    return df_moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df_data_input, user=None, random_state=42, percentage=None, weight = None, varianza = False, \n",
    "                        singolo = False, features_list=None, all_positions_list=None):\n",
    "\n",
    "    df_train = df_data_input[df_data_input['Userid'] != user].reset_index(drop=True)\n",
    "    df_test = df_data_input[df_data_input['Userid'] == user].reset_index(drop=True)\n",
    "\n",
    "    if len(all_positions_list) > 1:\n",
    "        if not singolo: \n",
    "            df_train = get_features_for_each_sensor_optimized(df_train[features_list + ['position', 'label', 'Userid','Activity']], all_positions_list)\n",
    "        df_test  = get_features_for_each_sensor_optimized(df_test[features_list + ['position', 'label', 'Userid','Activity']], all_positions_list)\n",
    "\n",
    "    df_test, df_testFISSO = train_test_split(df_test, test_size=0.2, random_state=random_state,stratify=df_test['label'])\n",
    "\n",
    "    #sposto le righe\n",
    "    righe_mosse = 0\n",
    "    moved_indices = []\n",
    "    if percentage > 0.0:\n",
    "        for label_value in df_test['label'].unique():\n",
    "            df_test_label = df_test[df_test['label'] == label_value]\n",
    "            num_to_move = max(1,int(len(df_test_label) * percentage))\n",
    "            indices_to_move = df_test_label.sample(n=num_to_move, random_state=random_state).index.tolist()\n",
    "            moved_indices.extend(indices_to_move)\n",
    "        righe_mosse = len(moved_indices)\n",
    "\n",
    "    if moved_indices:\n",
    "        df_moved = df_test.loc[moved_indices].copy()\n",
    "        if varianza and weight > 1:\n",
    "            df_moved = duplicaRighePesi(df_moved, weight)\n",
    "        if not singolo:\n",
    "            df_train = pd.concat([df_train, df_moved], ignore_index=True).reset_index(drop=True)\n",
    "        else:\n",
    "            df_train = df_moved.copy().reset_index(drop=True)\n",
    "\n",
    "    if len(all_positions_list) > 1:\n",
    "        X_train = df_train.drop(columns=['label','Userid'])\n",
    "        X_test = df_testFISSO.drop(columns=['label','Userid'])\n",
    "    else:\n",
    "        X_train = df_train[features_list]\n",
    "        X_test = df_testFISSO[features_list]\n",
    "    y_train = df_train['label']\n",
    "    y_test  = df_testFISSO['label']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, righe_mosse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, y_train, random_state, dove_peso):\n",
    "    hoSpostato = 0\n",
    "    class_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    final_sample_weights = dove_peso * class_weights\n",
    "    \n",
    "    try:\n",
    "        X_train_fold, X_eval_fold, y_train_fold, y_eval_fold, peso_fold, peso_eval = \\\n",
    "            train_test_split( X_train, y_train, final_sample_weights, test_size=0.1, random_state=random_state, stratify=y_train)\n",
    "        hoSpostato = len(X_eval_fold)\n",
    "        xgb = XGBClassifier( n_estimators=150, random_state=random_state, n_jobs=-1,early_stopping_rounds=10 )\n",
    "        xgb.fit(X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_eval_fold, y_eval_fold)],\n",
    "            sample_weight = peso_fold,\n",
    "            sample_weight_eval_set = [peso_eval], \n",
    "            verbose=False)\n",
    "    except: #quanto ho troppi pochi dati(eg percentuale < 10/15%)\n",
    "        xgb = XGBClassifier( n_estimators=150, random_state=random_state, n_jobs=-1)\n",
    "        xgb.fit(X_train, y_train, sample_weight = final_sample_weights)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    return y_pred, hoSpostato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prendiMax(df, position_key, random_states_list):\n",
    "    if not isinstance(random_states_list, list):\n",
    "        random_states_list = [random_states_list]\n",
    "    df_pos_filtered = df[(df['position'] == position_key) & (df['randomState'].isin(random_states_list))]\n",
    "    if df_pos_filtered.empty: return {} # Dizionario vuoto se non ci sono dati\n",
    "    grouped = df_pos_filtered.groupby(['randomState', 'timeUsed'])['f1-score'].mean()\n",
    "    max_medie = grouped.groupby('randomState').max()\n",
    "    return max_medie.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_random_state(rand_state, current_position_list, df_data_arg, weight_list_arg,\n",
    "                                lista_percentuali_arg, varianza_arg, singolo_arg, f1_s_max_dict_arg,\n",
    "                                baseCalcolata_arg, ROW_TIME_arg):\n",
    "    print(f\"Avvio random_state: {rand_state}, posizione: {','.join(current_position_list)}\")\n",
    "    single_state_results = []\n",
    "    all_sensors_flag = len(current_position_list) > 1\n",
    "\n",
    "    f1_s_max_for_pos_rstate_dict = None\n",
    "    current_pos_key_str = 'all sensors' if all_sensors_flag else current_position_list[0]\n",
    "    if baseCalcolata_arg and current_pos_key_str in f1_s_max_dict_arg:\n",
    "        f1_s_max_for_pos_rstate_dict = f1_s_max_dict_arg[current_pos_key_str]\n",
    "\n",
    "    all_original_features = [item for item in df_data_arg.columns if\n",
    "                             item not in ['Timestamp', 'Userid', 'UserAge', 'UserSex', 'UserHeight', 'UserWeight', 'Activity',\n",
    "                                          'position', 'label', 'MagnxEnergy', 'MagnyEnergy', 'MagnzEnergy', 'MagnMagnitude',\n",
    "                                          'MagnMagnitudeMean', 'MagnMagnitudeMin', 'MagnMagnitudeMax', 'MagnMagnitudeStd',\n",
    "                                          'MagnMagnitudeEnergy']]\n",
    "    all_original_features = [item for item in all_original_features if not re.match(r'.*MagnMagnitude.*', item)]\n",
    "    selected_original_features = [item for item in all_original_features if re.match(r'.*Magnitude.*', item)]\n",
    "\n",
    "\n",
    "    df_position = df_data_arg[df_data_arg['position'].isin(current_position_list)]\n",
    "    labels = df_data_arg['Activity'].unique()\n",
    "\n",
    "    for peso_arg in weight_list_arg:\n",
    "        stop_percentage_loop_for_this_combo = False\n",
    "        for percentuale_nuovo_train_arg in lista_percentuali_arg:\n",
    "            if singolo_arg and percentuale_nuovo_train_arg*100 == 0:\n",
    "                continue;\n",
    "            print(f\"  {rand_state} | P:{','.join(current_position_list)} | W:{peso_arg} | %:{int(percentuale_nuovo_train_arg*100)}\")\n",
    "            results_this_percentage_all_k = []\n",
    "            for k_user in df_position['Userid'].unique():\n",
    "                X_train, X_test, y_train, y_test, num_dati_spostati = \\\n",
    "                    get_train_test_data(\n",
    "                        df_position,\n",
    "                        user=k_user, random_state=rand_state,\n",
    "                        percentage=float(percentuale_nuovo_train_arg),\n",
    "                        weight=float(peso_arg), varianza=varianza_arg, singolo=singolo_arg,\n",
    "                        features_list=selected_original_features,\n",
    "                        all_positions_list=current_position_list\n",
    "                    )\n",
    "                if X_train.empty or X_test.empty or y_train.empty or y_test.empty:\n",
    "                    print(\"errore che non vuoin avere\")\n",
    "                    continue\n",
    "\n",
    "                if not varianza_arg and peso_arg > 1:\n",
    "                    len_train = len(X_train) - num_dati_spostati\n",
    "                    sample_weight = [1] * len_train + [peso_arg] * num_dati_spostati\n",
    "                else:\n",
    "                    sample_weight = [1] * len(X_train)\n",
    "                \n",
    "                start = time.perf_counter()\n",
    "                y_pred, hoSpostato = train_model(X_train, X_test, y_train, random_state=rand_state,dove_peso=sample_weight)\n",
    "                end = time.perf_counter()\n",
    "                durata = end - start\n",
    "\n",
    "                if hoSpostato > 0:\n",
    "                    num_dati_spostati = num_dati_spostati - hoSpostato\n",
    "\n",
    "                if len(y_test) == 0 or len(y_pred) == 0:\n",
    "                    print(\"errore che non vuoi avere\") #crasha tutto\n",
    "                    macro_f1 = np.nan; macro_precision = np.nan; macro_recall = np.nan\n",
    "                else:\n",
    "                    class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "                    macro_f1 = class_report.get('macro avg', {}).get('f1-score', np.nan)\n",
    "                    macro_precision = class_report.get('macro avg', {}).get('precision', np.nan)\n",
    "                    macro_recall = class_report.get('macro avg', {}).get('recall', np.nan)\n",
    "\n",
    "                current_metrics = {\n",
    "                    'k_user': k_user,\n",
    "                    'timeUsed': num_dati_spostati * ROW_TIME_arg,\n",
    "                    'percentage': int(percentuale_nuovo_train_arg * 100),\n",
    "                    'weight': float(peso_arg),\n",
    "                    'time': round(durata, 2),\n",
    "                    'randomState': rand_state,\n",
    "                    'position': current_pos_key_str,\n",
    "                    'f1-score': macro_f1,\n",
    "                    'precision': macro_precision, \n",
    "                    'recall': macro_recall\n",
    "                }\n",
    "\n",
    "                    \n",
    "                for label_idx, label in zip(y_train.unique(), labels):\n",
    "                    current_metrics[f'f1_{label}'] =  class_report[str(label_idx)]['f1-score']\n",
    "                    current_metrics[f'precision_{label}'] = class_report[str(label_idx)]['precision']\n",
    "                    current_metrics[f'recall_{label}'] = class_report[str(label_idx)]['recall']\n",
    "\n",
    "                results_this_percentage_all_k.append(current_metrics)\n",
    "\n",
    "            if baseCalcolata_arg and results_this_percentage_all_k and f1_s_max_for_pos_rstate_dict is not None:\n",
    "                df_temp_results = pd.DataFrame(results_this_percentage_all_k)\n",
    "                current_avg_f1 = df_temp_results['f1-score'].mean()\n",
    "                current_f1_s_max_this_rstate = f1_s_max_for_pos_rstate_dict.get(rand_state, np.nan)\n",
    "\n",
    "                if np.isfinite(current_avg_f1) and np.isfinite(current_f1_s_max_this_rstate):\n",
    "                    if current_avg_f1 >= current_f1_s_max_this_rstate:\n",
    "                        print(f\"    STOP a {int(percentuale_nuovo_train_arg*100)}% per peso {peso_arg}, r_state {rand_state}, pos {current_pos_key_str}\")\n",
    "                        stop_percentage_loop_for_this_combo = True\n",
    "            \n",
    "            single_state_results.extend(results_this_percentage_all_k)\n",
    "\n",
    "            if stop_percentage_loop_for_this_combo:\n",
    "                break\n",
    "    print(f\"Completata elaborazione per random_state: {rand_state}, posizione: {','.join(current_position_list)}\")\n",
    "    return single_state_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_parallel(\n",
    "    position_list_arg,\n",
    "    df_data_arg,\n",
    "    f1_s_max_dict_param,\n",
    "    baseCalcolata_param,\n",
    "    weight_list_param=None,\n",
    "    varianza_param=False,\n",
    "    singolo_param=False,\n",
    "    random_state_list_global=None,\n",
    "    ROW_TIME_global=None\n",
    "    ):\n",
    "\n",
    "    lista_percentuali_param = [i / 100 for i in range(0, 101, 5)]\n",
    "\n",
    "\n",
    "    ### DA USARE SOLO PER TESTING\n",
    "    # ci vedo i print + le prestazioni commentate sotto\n",
    "\n",
    "    # x = pd.DataFrame()\n",
    "    # for rs in random_state_list_global:\n",
    "    #     results_test_run = process_single_random_state(    \n",
    "    #             rs,\n",
    "    #             position_list_arg,\n",
    "    #             df_data_arg,\n",
    "    #             weight_list_param,\n",
    "    #             lista_percentuali_param,\n",
    "    #             varianza_param,\n",
    "    #             singolo_param,\n",
    "    #             f1_s_max_dict_param,\n",
    "    #             baseCalcolata_param,\n",
    "    #             ROW_TIME_global\n",
    "    #     )\n",
    "    #     y = pd.DataFrame(results_test_run)\n",
    "    #     x = pd.concat([x, y], axis=1)\n",
    "    # return x\n",
    "\n",
    "\n",
    "\n",
    "    num_cores = os.cpu_count()\n",
    "    print(f\"Posizione: {','.join(position_list_arg)}\")\n",
    "\n",
    "    parallel_outputs = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_single_random_state)(\n",
    "            rs,\n",
    "            position_list_arg,\n",
    "            df_data_arg,\n",
    "            weight_list_param,\n",
    "            lista_percentuali_param,\n",
    "            varianza_param,\n",
    "            singolo_param,\n",
    "            f1_s_max_dict_param,\n",
    "            baseCalcolata_param,\n",
    "            ROW_TIME_global\n",
    "        ) for rs in random_state_list_global\n",
    "    )\n",
    "\n",
    "    all_results_list = [item for sublist in parallel_outputs for item in sublist]\n",
    "    \n",
    "    if not all_results_list:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.DataFrame(all_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d318c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE_LIST = [int(i*11) for i in range(10,13)]\n",
    "ROW_TIME = 4\n",
    "SAVE = True\n",
    "WEIGHT_LIST = [5,10,25]\n",
    "COMBINED_POSITIONS = False\n",
    "\n",
    "DATASET = 'selfBACK' #'MultiPositionWearable', 'selfBACK', '?'\n",
    "\n",
    "if DATASET == 'MultiPositionWearable':\n",
    "  NOMI_FILE = {\n",
    "    'baseline_crowd': 'MultiPositionWearable_baseline_crowd',\n",
    "    'modello_base_crowd': 'MultiPositionWearable_base_crowd',\n",
    "    'modello_varianza_crowd': 'MultiPositionWearable_varianza_crowd',\n",
    "    'cartella_dati': 'MultiPositionWearable_processed_data'\n",
    "  }\n",
    "elif DATASET == 'selfBACK':\n",
    "  NOMI_FILE = {\n",
    "    'baseline_crowd': 'selfBACK_baseline_crowd',\n",
    "    'modello_base_crowd': 'selfBACK_base_crowd',\n",
    "    'modello_varianza_crowd': 'selfBACK_varianza_crowd',\n",
    "    'cartella_dati': 'selfBACK_processed_data'\n",
    "  }\n",
    "elif DATASET == '?':\n",
    "  NOMI_FILE = {\n",
    "    'baseline': '?_baseline',\n",
    "    'modello_base': '?_base',\n",
    "    'modello_varianza': '?_varianza',\n",
    "    'cartella_dati': '?_processed_data'\n",
    "  }\n",
    "else:\n",
    "    raise ValueError(\"DATASET non valido. Scegliere tra 'MultiPositionWearable', 'selfBACK' o '?'\")\n",
    "\n",
    "mypath_carica = os.getcwd() + '/data/' + NOMI_FILE['cartella_dati'] + '/'\n",
    "file_pattern = os.path.join(mypath_carica, 'grouped_data*.csv')\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "# Filtra i file che non contengono '_combined'\n",
    "file_list = [f for f in all_files if '_combined' not in os.path.basename(f)]\n",
    "\n",
    "df_data = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_temp = pd.read_csv(file, header=0)\n",
    "    if df_temp.columns[0].lower() in ['unnamed: 0', 'unnamed: 0.1']:\n",
    "      df_temp = df_temp.iloc[:, 1:]\n",
    "    df_data = pd.concat([df_data, df_temp], ignore_index=True)\n",
    "\n",
    "def set_labels(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['Activity'])\n",
    "    return df, label_encoder.classes_\n",
    "df_data, labels_activity_names = set_labels(df_data)\n",
    "def strip_Spaces(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "df_data = strip_Spaces(df_data)\n",
    "\n",
    "\n",
    "\n",
    "if len(all_files) - len(file_list) > 0:\n",
    "  COMBINED_POSITIONS = True\n",
    "  file_list = [f for f in all_files if '_combined' in os.path.basename(f)]\n",
    "  df_data_combined = pd.DataFrame()\n",
    "  for file in file_list:\n",
    "      df_temp = pd.read_csv(file, header=0)\n",
    "      if df_temp.columns[0].lower() in ['unnamed: 0', 'unnamed: 0.1']:\n",
    "        df_temp = df_temp.iloc[:, 1:]\n",
    "      df_data_combined = pd.concat([df_data_combined, df_temp], ignore_index=True)\n",
    "  df_data_combined, labels_activity_names_combined = set_labels(df_data_combined)\n",
    "  df_data_combined = strip_Spaces(df_data_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseCalcolata = False\n",
    "f1_s_max_dict = {}\n",
    "\n",
    "mypath_data_total = os.getcwd() + '/data/data_total/'\n",
    "os.makedirs(mypath_data_total, exist_ok=True)\n",
    "path_baseline_csv = mypath_data_total + NOMI_FILE['baseline_crowd'] + '.csv'\n",
    "\n",
    "list_df_baseline_parts = []\n",
    "\n",
    "if os.path.exists(path_baseline_csv) and SAVE:\n",
    "    print(f\"Caricamento baseline da: {path_baseline_csv}\")\n",
    "    df_f1_score_baseline = pd.read_csv(path_baseline_csv)\n",
    "else:\n",
    "    # DA USARE SOLO PER TESTING\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    print(\"Calcolo baseline (peso=1)...\")\n",
    "    \n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "    \n",
    "    print(\"Processing baseline per: all sensors (combinato)\")\n",
    "\n",
    "    if COMBINED_POSITIONS:\n",
    "        position_list = df_data_combined['position'].unique().tolist()\n",
    "        data = df_data_combined\n",
    "    else:\n",
    "        position_list = all_available_positions_from_df\n",
    "        data = df_data\n",
    "\n",
    "    df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "        position_list, data, {}, False, weight_list_param=[1],\n",
    "        random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME\n",
    "    )\n",
    "\n",
    "    list_df_baseline_parts.append(df_all_sensors_model)\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Processing baseline per: {pos_single}\")\n",
    "        df_pos_single_baseline = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data, {}, False, weight_list_param=[1],\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME\n",
    "        )\n",
    "        list_df_baseline_parts.append(df_pos_single_baseline)\n",
    "    \n",
    "    # DA USARE SOLO PER TESTING\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative') # o 'tottime'\n",
    "    # stats.print_stats(30) # Stampa le 30 funzioni più costose\n",
    "    \n",
    "    if list_df_baseline_parts:\n",
    "        df_f1_score_baseline = pd.concat(list_df_baseline_parts, ignore_index=True)\n",
    "        if SAVE:\n",
    "            df_f1_score_baseline.to_csv(path_baseline_csv, index=False)\n",
    "    else:\n",
    "        df_f1_score_baseline = pd.DataFrame() # Vuoto se nessun risultato\n",
    "\n",
    "if not df_f1_score_baseline.empty:\n",
    "    for pos_key_loop in df_f1_score_baseline['position'].unique():\n",
    "        f1_s_max_dict[pos_key_loop] = prendiMax(df_f1_score_baseline, pos_key_loop, RANDOM_STATE_LIST)\n",
    "        print(f\"Baseline f1-score {pos_key_loop}: {f1_s_max_dict[pos_key_loop]}\")\n",
    "else:\n",
    "    print(\"Attenzione: df_f1_score_baseline è vuoto. Impossibile popolare f1_s_max_dict.\")\n",
    "\n",
    "\n",
    "pesoBaseData = df_f1_score_baseline.copy()\n",
    "baseCalcolata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3731d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_base_csv = mypath_data_total + NOMI_FILE['modello_base_crowd'] + '.csv'\n",
    "if os.path.exists(path_model_base_csv) and SAVE:\n",
    "    print(f\"Modello base già calcolato, caricamento da: {path_model_base_csv}\")\n",
    "    baseData = pd.read_csv(path_model_base_csv)\n",
    "else:    \n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    print(\"Calcolo modello_base (pesi diversi, no varianza)...\")\n",
    "    list_df_model_base_parts = []\n",
    "\n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "\n",
    "    print(\"Processing baseline per: all sensors (combinato)\")\n",
    "\n",
    "    if COMBINED_POSITIONS:\n",
    "        position_list = df_data_combined['position'].unique().tolist()\n",
    "        data = df_data_combined\n",
    "    else:\n",
    "        position_list = all_available_positions_from_df\n",
    "        data = df_data\n",
    "\n",
    "    df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "        position_list, data, f1_s_max_dict, baseCalcolata,\n",
    "        weight_list_param=WEIGHT_LIST, varianza_param=False, singolo_param=False,\n",
    "        random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME\n",
    "    )\n",
    "\n",
    "    list_df_model_base_parts.append(df_all_sensors_model)\n",
    "\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Processing modello_base per: {pos_single}\")\n",
    "        df_pos_single_model = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data, f1_s_max_dict, baseCalcolata,\n",
    "            weight_list_param=WEIGHT_LIST, varianza_param=False, singolo_param=False,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME\n",
    "        )\n",
    "        list_df_model_base_parts.append(df_pos_single_model)\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative') # o 'tottime'\n",
    "    # stats.print_stats(30) # Stampa le 30 funzioni più costose\n",
    "    if list_df_model_base_parts:\n",
    "        df_model_base_results = pd.concat(list_df_model_base_parts, ignore_index=True)\n",
    "        baseData = pd.concat([df_model_base_results, pesoBaseData]).drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        baseData = pesoBaseData.copy()\n",
    "\n",
    "    if SAVE:\n",
    "        baseData.to_csv(path_model_base_csv, index=False)\n",
    "\n",
    "for pos_key_loop in baseData['position'].unique():\n",
    "    f1_s_max_dict[pos_key_loop] = prendiMax(baseData, pos_key_loop, RANDOM_STATE_LIST)\n",
    "    print(f\"Baseline f1-score {pos_key_loop}: {f1_s_max_dict[pos_key_loop]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_varianza_csv = mypath_data_total + NOMI_FILE['modello_varianza_crowd'] + '.csv'\n",
    "\n",
    "if os.path.exists(path_model_varianza_csv) and SAVE:\n",
    "    print(f\"Modello varianza già calcolato, caricamento da: {path_model_varianza_csv}\")\n",
    "    varianzaData = pd.read_csv(path_model_varianza_csv)\n",
    "else:\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    # print(\"Calcolo modello_varianza...\")\n",
    "    list_df_model_varianza_parts = []\n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "\n",
    "    print(\"Processing modello_varianza per: all sensors (combinato)\")\n",
    "    if COMBINED_POSITIONS:\n",
    "        position_list = df_data_combined['position'].unique().tolist()\n",
    "        data = df_data_combined\n",
    "    else:\n",
    "        position_list = all_available_positions_from_df\n",
    "        data = df_data\n",
    "\n",
    "    df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "        position_list, data, f1_s_max_dict, baseCalcolata,\n",
    "        weight_list_param=WEIGHT_LIST, varianza_param=True, singolo_param=False,\n",
    "        random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME\n",
    "    )\n",
    "    list_df_model_varianza_parts.append(df_all_sensors_model)\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Processing modello_varianza per: {pos_single}\")\n",
    "        df_pos_single_varianza = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data, f1_s_max_dict, baseCalcolata,\n",
    "            weight_list_param=WEIGHT_LIST, varianza_param=True, singolo_param=False,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME\n",
    "        )\n",
    "        list_df_model_varianza_parts.append(df_pos_single_varianza)\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative') # o 'tottime'\n",
    "    # stats.print_stats(30) # Stampa le 30 funzioni più costose\n",
    "    if list_df_model_varianza_parts:\n",
    "        df_model_varianza_results = pd.concat(list_df_model_varianza_parts, ignore_index=True)\n",
    "        varianzaData = pd.concat([df_model_varianza_results, pesoBaseData]).drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        varianzaData = pesoBaseData.copy()\n",
    "\n",
    "    if SAVE:\n",
    "        varianzaData.to_csv(path_model_varianza_csv, index=False)\n",
    "        \n",
    "for pos_key_loop in varianzaData['position'].unique():\n",
    "    f1_s_max_dict[pos_key_loop] = prendiMax(varianzaData, pos_key_loop, RANDOM_STATE_LIST)\n",
    "    print(f\"Baseline f1-score {pos_key_loop}: {f1_s_max_dict[pos_key_loop]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

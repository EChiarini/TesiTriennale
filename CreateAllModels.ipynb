{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from joblib import Parallel, delayed\n",
    "from genericpath import isfile\n",
    "from ntpath import join\n",
    "from os import listdir\n",
    "import glob\n",
    "import cProfile, pstats # per vedere quanti ci metto\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa23d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_each_sensor_optimized(df_data_input, positions_list):\n",
    "    if not positions_list or df_data_input.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    grouping_keys = ['Userid', 'label']\n",
    "    # Colonne da escludere dalla ridenominazione delle feature\n",
    "    meta_cols_to_exclude_from_rename = ['label', 'position', 'Userid','Activity']\n",
    "\n",
    "    all_processed_group_dfs = []\n",
    "    # Itera su ogni gruppo (Userid, label)\n",
    "    for (uid, lbl), group_df in df_data_input.groupby(grouping_keys, observed=False):\n",
    "        processed_dfs_for_group_concat = []\n",
    "        labels_columns_for_mask_group = []\n",
    "        \n",
    "        min_len_for_group_alignment = float('inf')\n",
    "        temp_pos_dfs_for_group = {}\n",
    "\n",
    "        #trova la lunghezza minima tra le posizioni\n",
    "        actual_positions_in_group = [p for p in positions_list if p in group_df['position'].unique()]\n",
    "        if len(actual_positions_in_group) != len(positions_list):\n",
    "            #non vuoi entrare qui\n",
    "            continue \n",
    "\n",
    "        for position_val in actual_positions_in_group: # Usa actual_positions_in_group\n",
    "            df_pos_filtered_group = group_df[group_df['position'] == position_val]\n",
    "            temp_pos_dfs_for_group[position_val] = df_pos_filtered_group\n",
    "            if len(df_pos_filtered_group) < min_len_for_group_alignment:\n",
    "                min_len_for_group_alignment = len(df_pos_filtered_group)\n",
    "        \n",
    "        if min_len_for_group_alignment == 0 or min_len_for_group_alignment == float('inf'):\n",
    "            #non vuoi entrare qui\n",
    "            continue\n",
    "\n",
    "        for position_val in actual_positions_in_group:\n",
    "            df_pos_segment_group = temp_pos_dfs_for_group[position_val].head(min_len_for_group_alignment).reset_index(drop=True)\n",
    "\n",
    "            cols_to_rename_group = [col for col in df_pos_segment_group.columns if col not in meta_cols_to_exclude_from_rename]\n",
    "            rename_dict_group = {col: f\"{col}_{position_val}\" for col in cols_to_rename_group}\n",
    "            \n",
    "            df_features_renamed_group = df_pos_segment_group.rename(columns=rename_dict_group)[list(rename_dict_group.values())]\n",
    "\n",
    "            label_col_name_for_mask_group = f'label_{position_val}' # Usato solo per il check mask\n",
    "            df_label_for_mask_group = df_pos_segment_group[['label']].rename(columns={'label': label_col_name_for_mask_group})\n",
    "            labels_columns_for_mask_group.append(label_col_name_for_mask_group)\n",
    "            \n",
    "            current_df_part_group = pd.concat([df_features_renamed_group, df_label_for_mask_group], axis=1)\n",
    "            processed_dfs_for_group_concat.append(current_df_part_group)\n",
    "\n",
    "        if not processed_dfs_for_group_concat:\n",
    "            continue\n",
    "        \n",
    "        df_group_final_wide = pd.concat(processed_dfs_for_group_concat, axis=1)\n",
    "\n",
    "        # Applica la maschera per le label\n",
    "        existing_labels_cols_group = [col for col in labels_columns_for_mask_group if col in df_group_final_wide.columns]\n",
    "        if not existing_labels_cols_group: continue # Non dovrebbe succedere\n",
    "\n",
    "        if len(existing_labels_cols_group) > 1:\n",
    "            label_data_group = df_group_final_wide[existing_labels_cols_group]\n",
    "            mask_group = label_data_group.nunique(axis=1, dropna=True) <= 1\n",
    "            df_group_filtered_wide = df_group_final_wide[mask_group]\n",
    "        else: # Solo una colonna label, nessun filtro mask necessario\n",
    "            df_group_filtered_wide = df_group_final_wide\n",
    "        \n",
    "        if df_group_filtered_wide.empty: continue\n",
    "\n",
    "        # Rinomina la prima colonna label in 'label' e droppa le altre (ridondanti)\n",
    "        df_group_filtered_wide = df_group_filtered_wide.rename(columns={existing_labels_cols_group[0]: 'label'})\n",
    "        if len(existing_labels_cols_group) > 1:\n",
    "            cols_to_drop_labels_group = [col for col in existing_labels_cols_group[1:] if col in df_group_filtered_wide.columns]\n",
    "            if cols_to_drop_labels_group:\n",
    "                 df_group_filtered_wide = df_group_filtered_wide.drop(columns=cols_to_drop_labels_group)\n",
    "\n",
    "        df_group_filtered_wide['Userid'] = uid\n",
    "        all_processed_group_dfs.append(df_group_filtered_wide)\n",
    "\n",
    "    if not all_processed_group_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    final_combined_df = pd.concat(all_processed_group_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c09c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicaRighePesi(df_moved, weight): #solo per varianza\n",
    "    if df_moved.empty: return df_moved\n",
    "    df_moved['is_original'] = True\n",
    "    repeated_part = df_moved.loc[np.repeat(df_moved.index, int(weight) - 1)].copy()\n",
    "    repeated_part['is_original'] = False\n",
    "    df_moved = pd.concat([df_moved, repeated_part], ignore_index=True)\n",
    "    feature_cols = df_moved.columns.difference(['label', 'is_original', 'Userid', 'Timestamp', 'Activity', 'position'])\n",
    "    feature_cols = df_moved[feature_cols].select_dtypes(include=[np.number]).columns\n",
    "    if not feature_cols.empty:\n",
    "        df_moved[feature_cols] = df_moved[feature_cols].astype(float)\n",
    "        df_moved.loc[~df_moved['is_original'], feature_cols] *= np.random.uniform(0.99, 1.01, size=df_moved.loc[~df_moved['is_original'], feature_cols].shape)\n",
    "    df_moved = df_moved.drop('is_original', axis=1, errors='ignore')\n",
    "    return df_moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df_data_input, user=None, random_state=42, weight = -1, varianza = False, \n",
    "                        features_list=None, all_positions_list=None, row_to_move = 0):\n",
    "\n",
    "    df_train = df_data_input[df_data_input['Userid'] != user].reset_index(drop=True)\n",
    "    df_test = df_data_input[df_data_input['Userid'] == user].reset_index(drop=True)\n",
    "\n",
    "    if len(all_positions_list) > 1:\n",
    "        df_train = get_features_for_each_sensor_optimized(df_train[features_list + ['position', 'label', 'Userid','Activity']], all_positions_list)\n",
    "        df_test  = get_features_for_each_sensor_optimized(df_test[features_list + ['position', 'label', 'Userid','Activity']], all_positions_list)\n",
    "    df_sampling_pool, df_testFISSO = train_test_split(df_test, test_size=0.2, random_state=random_state,stratify=df_test['label'])\n",
    "\n",
    "    #sposto le righe\n",
    "    num_to_move = 0\n",
    "    moved_indices = []\n",
    "    if row_to_move > 0:\n",
    "        for label_value in df_sampling_pool['label'].unique():\n",
    "            df_test_label = df_sampling_pool[df_sampling_pool['label'] == label_value]\n",
    "            indices_to_move = df_test_label.sample(n=row_to_move, random_state=random_state).index.tolist()\n",
    "            moved_indices.extend(indices_to_move)\n",
    "\n",
    "    total_rows_moved = len(moved_indices)\n",
    "\n",
    "    if moved_indices:\n",
    "        df_moved = df_sampling_pool.loc[moved_indices].copy()\n",
    "        if varianza and weight > 1:\n",
    "            df_moved = duplicaRighePesi(df_moved, weight)\n",
    "        df_train = pd.concat([df_train, df_moved], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    if len(all_positions_list) > 1:\n",
    "        X_train = df_train.drop(columns=['label','Userid'])\n",
    "        X_test = df_testFISSO.drop(columns=['label','Userid'])\n",
    "    else:\n",
    "        X_train = df_train[features_list]\n",
    "        X_test = df_testFISSO[features_list]\n",
    "    y_train = df_train['label']\n",
    "    y_test  = df_testFISSO['label']\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, total_rows_moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, y_train, random_state, dove_peso):\n",
    "    class_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    final_sample_weights = dove_peso * class_weights\n",
    "\n",
    "    xgb = XGBClassifier( n_estimators=150, random_state=random_state, n_jobs=-1)\n",
    "    xgb.fit(X_train, y_train, sample_weight = final_sample_weights)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_random_state(rand_state, current_position_list, df_data_arg, weight_list_arg,\n",
    "                                lista_minuti_arg, varianza_arg, ROW_TIME_arg, OVERLAP_arg):\n",
    "    print(f\"Avvio random_state: {rand_state}, posizione: {','.join(current_position_list)}\")\n",
    "    single_state_results = []\n",
    "    all_sensors_flag = len(current_position_list) > 1\n",
    "    current_pos_key_str = 'all sensors' if all_sensors_flag else current_position_list[0]\n",
    "\n",
    "    all_original_features = [item for item in df_data_arg.columns if\n",
    "                             item not in ['Timestamp', 'Userid', 'UserAge', 'UserSex', 'UserHeight', 'UserWeight', 'Activity',\n",
    "                                          'position', 'label', 'MagnxEnergy', 'MagnyEnergy', 'MagnzEnergy', 'MagnMagnitude',\n",
    "                                          'MagnMagnitudeMean', 'MagnMagnitudeMin', 'MagnMagnitudeMax', 'MagnMagnitudeStd',\n",
    "                                          'MagnMagnitudeEnergy']]\n",
    "    all_original_features = [item for item in all_original_features if not re.match(r'.*MagnMagnitude.*', item)]\n",
    "    selected_original_features = [item for item in all_original_features if re.match(r'.*Magnitude.*', item)]\n",
    "\n",
    "    df_position = df_data_arg[df_data_arg['position'].isin(current_position_list)]\n",
    "    labels = df_data_arg['Activity'].unique()\n",
    "\n",
    "    if weight_list_arg is None:\n",
    "        crowd_results = []\n",
    "        for k_user in df_position['Userid'].unique():\n",
    "            X_train, X_test, y_train, y_test, num_dati_spostati = \\\n",
    "                            get_train_test_data(\n",
    "                                df_position,\n",
    "                                user=k_user, random_state=rand_state,\n",
    "                                features_list=selected_original_features,\n",
    "                                all_positions_list=current_position_list\n",
    "                            )\n",
    "\n",
    "            sample_weight = [1] * len(X_train)\n",
    "            y_pred = train_model(X_train, X_test, y_train, random_state=rand_state,dove_peso=sample_weight)\n",
    "\n",
    "            if len(y_test) == 0 or len(y_pred) == 0:\n",
    "                print(\"errore che non vuoi avere\") #crasha tutto\n",
    "                macro_f1 = np.nan; macro_precision = np.nan; macro_recall = np.nan\n",
    "            else:\n",
    "                class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "                macro_f1 = class_report.get('macro avg', {}).get('f1-score', np.nan)\n",
    "                macro_precision = class_report.get('macro avg', {}).get('precision', np.nan)\n",
    "                macro_recall = class_report.get('macro avg', {}).get('recall', np.nan)\n",
    "\n",
    "            current_metrics = {\n",
    "                'k_user': k_user,\n",
    "                'randomState': rand_state,\n",
    "                'position': current_pos_key_str,\n",
    "                'f1-score': macro_f1,\n",
    "                'precision': macro_precision, \n",
    "                'recall': macro_recall\n",
    "            }\n",
    "            crowd_results.append(current_metrics)\n",
    "\n",
    "        return crowd_results\n",
    "    else:\n",
    "        minimo_gruppo = df_position.groupby(['Userid', 'label', 'position']).size().min()\n",
    "        minimo_disponibile = math.floor(minimo_gruppo * 0.8)\n",
    "        for peso_arg in weight_list_arg:\n",
    "            for secondi_nuovo_train_arg in lista_minuti_arg:\n",
    "                target_n_total = 1 + (secondi_nuovo_train_arg - ROW_TIME_arg) / (ROW_TIME_arg * (1 - OVERLAP_arg))\n",
    "                num_ideale_per_classe = max(1, target_n_total)\n",
    "                final_num_to_move = int(min(num_ideale_per_classe, minimo_disponibile))\n",
    "                tempoEffettivo = ROW_TIME_arg * (1 + (final_num_to_move - 1) * (1 - OVERLAP_arg))\n",
    "                \n",
    "                results_this_minute_all_k = []\n",
    "                for k_user in df_position['Userid'].unique():\n",
    "                    X_train, X_test, y_train, y_test, num_dati_spostati = \\\n",
    "                        get_train_test_data(\n",
    "                            df_position,\n",
    "                            user=k_user, random_state=rand_state,\n",
    "                            weight=int(peso_arg), varianza=varianza_arg,\n",
    "                            features_list=selected_original_features,\n",
    "                            all_positions_list=current_position_list,\n",
    "                            row_to_move = final_num_to_move\n",
    "                        )\n",
    "                    if X_train.empty or X_test.empty or y_train.empty or y_test.empty:\n",
    "                        print(\"errore che non vuoi avere\")\n",
    "                        continue\n",
    "\n",
    "                    if not varianza_arg and peso_arg > 1:\n",
    "                        len_train = len(X_train) - num_dati_spostati\n",
    "                        sample_weight = [1] * len_train + [peso_arg] * num_dati_spostati\n",
    "                    else:\n",
    "                        sample_weight = [1] * len(X_train)\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    y_pred = train_model(X_train, X_test, y_train, random_state=rand_state,dove_peso=sample_weight)\n",
    "                    end = time.perf_counter()\n",
    "                    durata = end - start\n",
    "\n",
    "\n",
    "\n",
    "                    if len(y_test) == 0 or len(y_pred) == 0:\n",
    "                        print(\"errore che non vuoi avere\") #crasha tutto\n",
    "                        macro_f1 = np.nan; macro_precision = np.nan; macro_recall = np.nan\n",
    "                    else:\n",
    "                        class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "                        macro_f1 = class_report.get('macro avg', {}).get('f1-score', np.nan)\n",
    "                        macro_precision = class_report.get('macro avg', {}).get('precision', np.nan)\n",
    "                        macro_recall = class_report.get('macro avg', {}).get('recall', np.nan)\n",
    "\n",
    "                    current_metrics = {\n",
    "                        'k_user': k_user,\n",
    "                        'timeUsed': int(tempoEffettivo),\n",
    "                        'weight': int(peso_arg),\n",
    "                        'time': round(durata, 2),\n",
    "                        'randomState': rand_state,\n",
    "                        'position': current_pos_key_str,\n",
    "                        'f1-score': macro_f1,\n",
    "                        'precision': macro_precision, \n",
    "                        'recall': macro_recall\n",
    "                    }\n",
    "\n",
    "                    for label_idx, label in zip(y_train.unique(), labels):\n",
    "                        current_metrics[f'f1_{label}'] =  class_report[str(label_idx)]['f1-score']\n",
    "                        current_metrics[f'precision_{label}'] = class_report[str(label_idx)]['precision']\n",
    "                        current_metrics[f'recall_{label}'] = class_report[str(label_idx)]['recall']\n",
    "\n",
    "                    results_this_minute_all_k.append(current_metrics)\n",
    "\n",
    "                single_state_results.extend(results_this_minute_all_k)\n",
    "\n",
    "        return single_state_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_parallel(\n",
    "    position_list_arg,\n",
    "    df_data_arg,\n",
    "    weight_list_param=None,\n",
    "    varianza_param=False,\n",
    "    random_state_list_global=None,\n",
    "    ROW_TIME_global=None,\n",
    "    OVERLAP_global=None,\n",
    "    minute_list_global=None\n",
    "    ):\n",
    "\n",
    "    ### DA USARE SOLO PER TESTING\n",
    "    # ci vedo i print + le prestazioni commentate sotto\n",
    "\n",
    "    # x = pd.DataFrame()\n",
    "    # for rs in random_state_list_global:\n",
    "    #     results_test_run = process_single_random_state(    \n",
    "    #             rs,\n",
    "    #             position_list_arg,\n",
    "    #             df_data_arg,\n",
    "    #             weight_list_param,\n",
    "    #             minute_list_global,\n",
    "    #             varianza_param,\n",
    "    #             ROW_TIME_global,\n",
    "    #             OVERLAP_global\n",
    "    #     )\n",
    "    #     y = pd.DataFrame(results_test_run)\n",
    "    #     x = pd.concat([x, y], axis=1)\n",
    "    # return x\n",
    "\n",
    "    num_cores = os.cpu_count()\n",
    "\n",
    "    parallel_outputs = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_single_random_state)(\n",
    "            rs,\n",
    "            position_list_arg,\n",
    "            df_data_arg,\n",
    "            weight_list_param,\n",
    "            minute_list_global,\n",
    "            varianza_param,\n",
    "            ROW_TIME_global,\n",
    "            OVERLAP_global\n",
    "        ) for rs in random_state_list_global\n",
    "    )\n",
    "    all_results_list = [item for sublist in parallel_outputs for item in sublist]\n",
    "        \n",
    "    return pd.DataFrame(all_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d318c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        code = f.read()\n",
    "        local_vars = {}\n",
    "        exec(code, {}, local_vars)\n",
    "        return local_vars\n",
    "vars = load_variables_from_file('Configurazioni.txt')\n",
    "\n",
    "RANDOM_STATE_LIST = vars['RANDOM_STATE_LIST']\n",
    "SECONDS_LIST = vars['SECONDS_LIST']\n",
    "WEIGHT_LIST = vars['WEIGHT_LIST']\n",
    "DATASET = vars['DATASET']\n",
    "\n",
    "OVERLAP = 0.5\n",
    "ROW_TIME = 4\n",
    "SAVE = True\n",
    "COMBINED_POSITIONS = False\n",
    "\n",
    "if DATASET == 'selfBACK':\n",
    "  NOMI_FILE = {\n",
    "    'baseline_crowd': 'selfBACK_baseline_crowd',\n",
    "    'modello_base_crowd': 'selfBACK_base_crowd',\n",
    "    'modello_varianza_crowd': 'selfBACK_varianza_crowd',\n",
    "    'cartella_dati': 'selfBACK_processed_data'\n",
    "  }\n",
    "elif DATASET == 'SDALLE':\n",
    "  NOMI_FILE = {\n",
    "    'baseline_crowd': 'SDALLE_baseline_crowd',\n",
    "    'modello_base_crowd': 'SDALLE_base_crowd',\n",
    "    'modello_varianza_crowd': 'SDALLE_varianza_crowd',\n",
    "    'cartella_dati': 'SDALLE_processed_data'\n",
    "  }\n",
    "else:\n",
    "    raise ValueError(\"DATASET non valido. Scegliere tra 'selfBACK' o 'SDALLE'\")\n",
    "\n",
    "mypath_carica = os.getcwd() + '/data/' + NOMI_FILE['cartella_dati'] + '/'\n",
    "file_pattern = os.path.join(mypath_carica, 'grouped_data*.csv')\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "# Filtra i file che non contengono '_combined'\n",
    "file_list = [f for f in all_files if '_combined' not in os.path.basename(f)]\n",
    "\n",
    "df_data = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_temp = pd.read_csv(file, header=0)\n",
    "    if df_temp.columns[0].lower() in ['unnamed: 0', 'unnamed: 0.1']:\n",
    "      df_temp = df_temp.iloc[:, 1:]\n",
    "    df_data = pd.concat([df_data, df_temp], ignore_index=True)\n",
    "\n",
    "def set_labels(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['Activity'])\n",
    "    return df, label_encoder.classes_\n",
    "df_data, labels_activity_names = set_labels(df_data)\n",
    "def strip_Spaces(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "df_data = strip_Spaces(df_data)\n",
    "\n",
    "\n",
    "\n",
    "if len(all_files) - len(file_list) > 0:\n",
    "  COMBINED_POSITIONS = True\n",
    "  print(\"Dati pre-combinati\")\n",
    "  file_list = [f for f in all_files if '_combined' in os.path.basename(f)]\n",
    "  df_data_combined = pd.DataFrame()\n",
    "  for file in file_list:\n",
    "      df_temp = pd.read_csv(file, header=0)\n",
    "      if df_temp.columns[0].lower() in ['unnamed: 0', 'unnamed: 0.1']:\n",
    "        df_temp = df_temp.iloc[:, 1:]\n",
    "      df_data_combined = pd.concat([df_data_combined, df_temp], ignore_index=True)\n",
    "  df_data_combined, labels_activity_names_combined = set_labels(df_data_combined)\n",
    "  df_data_combined = strip_Spaces(df_data_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath_data_total = os.getcwd() + '/data/data_total/'\n",
    "os.makedirs(mypath_data_total, exist_ok=True)\n",
    "path_baseline_csv = mypath_data_total + NOMI_FILE['baseline_crowd'] + '.csv'\n",
    "\n",
    "list_df_baseline_parts = []\n",
    "\n",
    "if os.path.exists(path_baseline_csv) and SAVE:\n",
    "    print(f\"Caricamento baseline da: {path_baseline_csv}\")\n",
    "    pesoBaseData = pd.read_csv(path_baseline_csv)\n",
    "else:\n",
    "    # DA USARE SOLO PER TESTING\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()    \n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "    \n",
    "    if len(all_available_positions_from_df) > 1:\n",
    "        print(\"Calcolo baseline per: all sensors\")\n",
    "\n",
    "        if COMBINED_POSITIONS:\n",
    "            position_list = df_data_combined['position'].unique().tolist()\n",
    "            data = df_data_combined\n",
    "        else:\n",
    "            position_list = all_available_positions_from_df\n",
    "            data = df_data\n",
    "\n",
    "        df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "            position_list, data,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "\n",
    "        list_df_baseline_parts.append(df_all_sensors_model)\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Calcolo baseline per: {pos_single}\")\n",
    "        df_pos_single_baseline = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data, \n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "        list_df_baseline_parts.append(df_pos_single_baseline)\n",
    "    \n",
    "    # DA USARE SOLO PER TESTING\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "    # stats.print_stats(30)\n",
    "    \n",
    "    if list_df_baseline_parts:\n",
    "        pesoBaseData = pd.concat(list_df_baseline_parts, ignore_index=True)\n",
    "        if SAVE:\n",
    "            pesoBaseData.to_csv(path_baseline_csv, index=False)\n",
    "    else:\n",
    "        pesoBaseData = pd.DataFrame() # Vuoto se nessun risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3731d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_base_csv = mypath_data_total + NOMI_FILE['modello_base_crowd'] + '.csv'\n",
    "if os.path.exists(path_model_base_csv) and SAVE:\n",
    "    print(f\"Modello base già calcolato, caricamento da: {path_model_base_csv}\")\n",
    "    baseData = pd.read_csv(path_model_base_csv)\n",
    "else:    \n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    list_df_model_base_parts = []\n",
    "\n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "    if len(all_available_positions_from_df) > 1:\n",
    "        print(\"Calcolo base per: all sensors\")\n",
    "\n",
    "        if COMBINED_POSITIONS:\n",
    "            position_list = df_data_combined['position'].unique().tolist()\n",
    "            data = df_data_combined\n",
    "        else:\n",
    "            position_list = all_available_positions_from_df\n",
    "            data = df_data\n",
    "\n",
    "        df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "            position_list, data,\n",
    "            weight_list_param=WEIGHT_LIST,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "\n",
    "        list_df_model_base_parts.append(df_all_sensors_model)\n",
    "\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Calcolo base per: {pos_single}\")\n",
    "        df_pos_single_model = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data,\n",
    "            weight_list_param=WEIGHT_LIST,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "\n",
    "        list_df_model_base_parts.append(df_pos_single_model)\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "    # stats.print_stats(30)\n",
    "    baseData = pd.concat(list_df_model_base_parts, ignore_index=True)\n",
    "\n",
    "    if SAVE:\n",
    "        baseData.to_csv(path_model_base_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_varianza_csv = mypath_data_total + NOMI_FILE['modello_varianza_crowd'] + '.csv'\n",
    "\n",
    "if os.path.exists(path_model_varianza_csv) and SAVE:\n",
    "    print(f\"Modello varianza già calcolato, caricamento da: {path_model_varianza_csv}\")\n",
    "    varianzaData = pd.read_csv(path_model_varianza_csv)\n",
    "else:\n",
    "    # profiler = cProfile.Profile()\n",
    "    # profiler.enable()\n",
    "    # print(\"Calcolo modello_varianza...\")\n",
    "    list_df_model_varianza_parts = []\n",
    "    all_available_positions_from_df = list(df_data['position'].unique())\n",
    "    if len(all_available_positions_from_df) > 1:\n",
    "        print(\"Processing varianza per: all sensors (combinato)\")\n",
    "        if COMBINED_POSITIONS:\n",
    "            position_list = df_data_combined['position'].unique().tolist()\n",
    "            data = df_data_combined\n",
    "        else:\n",
    "            position_list = all_available_positions_from_df\n",
    "            data = df_data\n",
    "\n",
    "        df_all_sensors_model = k_fold_cross_validation_parallel(\n",
    "            position_list, data,\n",
    "            weight_list_param=WEIGHT_LIST, varianza_param=True,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "        list_df_model_varianza_parts.append(df_all_sensors_model)\n",
    "\n",
    "    for pos_single in all_available_positions_from_df:\n",
    "        print(f\"Processing varianza per: {pos_single}\")\n",
    "        df_pos_single_varianza = k_fold_cross_validation_parallel(\n",
    "            [pos_single], df_data,\n",
    "            weight_list_param=WEIGHT_LIST, varianza_param=True,\n",
    "            random_state_list_global=RANDOM_STATE_LIST, ROW_TIME_global=ROW_TIME,\n",
    "            OVERLAP_global=OVERLAP, minute_list_global=SECONDS_LIST\n",
    "        )\n",
    "        list_df_model_varianza_parts.append(df_pos_single_varianza)\n",
    "    # profiler.disable()\n",
    "    # stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "    # stats.print_stats(30)\n",
    "    varianzaData = pd.concat(list_df_model_varianza_parts, ignore_index=True)\n",
    "\n",
    "    if SAVE:\n",
    "        varianzaData.to_csv(path_model_varianza_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesitriennale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

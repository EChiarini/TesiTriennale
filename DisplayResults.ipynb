{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2590394af823fa4",
   "metadata": {},
   "source": [
    "### Dipendenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6eb117ab2cec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T08:12:45.126167Z",
     "start_time": "2025-04-29T08:12:45.113040Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76076d1848fc89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T08:12:45.221407Z",
     "start_time": "2025-04-29T08:12:45.209071Z"
    }
   },
   "outputs": [],
   "source": [
    "MYPATH = os.getcwd() + '/data/data_total/'\n",
    "TOLLERANCE = 0.01\n",
    "metric_to_compare = 'f1-score'\n",
    "time_column_name = 'time'\n",
    "timeUsed_column_name = 'timeUsed'\n",
    "random_state_column = 'randomState'\n",
    "datasets_to_process = ['XGB', 'VARIANZA']\n",
    "TEMPO = False\n",
    "MAGGIORE = True\n",
    "DATASET = 'default' #'default', 'selfBACK', '?'\n",
    "\n",
    "if DATASET == 'default':\n",
    "  NOMI_FILE = {\n",
    "    'baseline': 'modelXGBtotal_baseline',\n",
    "    'modello_base': 'modelXGBtotal_base',\n",
    "    'modello_varianza': 'modelXGBtotal_varianza',\n",
    "  }\n",
    "elif DATASET == 'selfBACK':\n",
    "  NOMI_FILE = {\n",
    "    'baseline': 'selfBACK_modelXGBtotal_baseline',\n",
    "    'modello_base': 'selfBACK_modelXGBtotal_base',\n",
    "    'modello_varianza': 'selfBACK_modelXGBtotal_varianza',\n",
    "  }\n",
    "elif DATASET == '?':\n",
    "  NOMI_FILE = {\n",
    "    'baseline': '?_modelXGBtotal_baseline',\n",
    "    'modello_base': '?_modelXGBtotal_base',\n",
    "    'modello_varianza': '?_modelXGBtotal_varianza',\n",
    "  }\n",
    "else:\n",
    "    raise ValueError(\"DATASET non valido. Scegliere tra 'default', 'selfBACK' o '?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82168769",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(MYPATH + NOMI_FILE['modello_base'] + '.csv', header=0)\n",
    "POSIZIONI = temp['position'].unique()\n",
    "PESI = temp['weight'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_seconds_to_minutes(val,pos):\n",
    "    if val < 60:\n",
    "        return f\"{val:.2f} s\"\n",
    "    else:\n",
    "        minutes = val / 60\n",
    "        return f\"{minutes:.2f} min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49351b17a4ebb5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T08:18:14.031566Z",
     "start_time": "2025-04-29T08:18:09.914436Z"
    }
   },
   "outputs": [],
   "source": [
    "for position_to_plot in POSIZIONI:\n",
    "    try:\n",
    "        df_singolo_full = pd.read_csv(MYPATH + NOMI_FILE['baseline'] + '.csv', header=0)\n",
    "        if df_singolo_full.empty:\n",
    "            raise FileNotFoundError(f\"Nessun dato SINGOLO (XGB peso=1) per {position_to_plot}\")\n",
    "\n",
    "        required_cols_singolo = [metric_to_compare, timeUsed_column_name, random_state_column]\n",
    "\n",
    "        df_pos = df_singolo_full[\n",
    "            (df_singolo_full['position'] == position_to_plot) &\n",
    "            (df_singolo_full['randomState'].isin(df_singolo_full['randomState'].unique()))\n",
    "        ]\n",
    "        grouped = df_pos.groupby(['randomState', 'timeUsed'])[metric_to_compare].mean()\n",
    "        max_medie = grouped.groupby('randomState').max()\n",
    "        baseline_max_f1_dict = max_medie.to_dict()\n",
    "\n",
    "        all_other_dfs = []\n",
    "        datasets_files = {\n",
    "            'XGB': MYPATH +  NOMI_FILE['modello_base'] + '.csv',\n",
    "            'VARIANZA': MYPATH +  NOMI_FILE['modello_varianza'] + '.csv',\n",
    "        }\n",
    "        temp_datasets_to_process = list(datasets_to_process)\n",
    "        for name, filepath in datasets_files.items():\n",
    "            if name in temp_datasets_to_process:\n",
    "                try:\n",
    "                    df_loaded = pd.read_csv(filepath, header=0)\n",
    "                    df_loaded['dataset'] = name\n",
    "                    df_loaded = df_loaded[df_loaded['position'] == position_to_plot]\n",
    "\n",
    "                    required_cols = ['weight', timeUsed_column_name, metric_to_compare, random_state_column]\n",
    "                    if not all(col in df_loaded.columns for col in required_cols):\n",
    "                        print(f\"Attenzione: Colonne mancanti in {filepath} ({name}). Salto.\")\n",
    "                        datasets_to_process.remove(name)\n",
    "                        continue\n",
    "\n",
    "                    cols_to_keep = ['weight', timeUsed_column_name, metric_to_compare, 'dataset', random_state_column]\n",
    "                    if time_column_name in df_loaded.columns:\n",
    "                        cols_to_keep.append(time_column_name)\n",
    "                    all_other_dfs.append(df_loaded[cols_to_keep])\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File non trovato {filepath} ({name})\")\n",
    "                    datasets_to_process.remove(name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore {filepath} ({name}) {e}\")\n",
    "                    datasets_to_process.remove(name)\n",
    "\n",
    "        df_all_others = pd.concat(all_other_dfs, ignore_index=True)\n",
    "        df_all_others = df_all_others[df_all_others['weight'].isin(PESI)]\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Errore caricamento file {e}\")\n",
    "        continue\n",
    "    except ValueError as e:\n",
    "        print(f\"Errore nei dati {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Errore generico caricamento dati {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Matching timeUsed per ogni run ---\n",
    "    results_individual = []\n",
    "    unique_random_states = df_all_others[random_state_column].unique()\n",
    "\n",
    "    for weight_val in PESI:\n",
    "        df_all_others_current_weight = df_all_others[df_all_others['weight'] == weight_val]\n",
    "\n",
    "        for dataset_name in datasets_to_process:\n",
    "            for r_state in unique_random_states:\n",
    "                current_f1_s_max = baseline_max_f1_dict.get(r_state, np.nan)\n",
    "                if not np.isfinite(current_f1_s_max):\n",
    "                    results_individual.append({ 'weight': weight_val, 'dataset': dataset_name, 'random_state': r_state, 'matching_timeUsed': np.nan, 'corresponding_time': np.nan }); continue\n",
    "\n",
    "                df_current_run = df_all_others_current_weight[ (df_all_others_current_weight['dataset'] == dataset_name) & (df_all_others_current_weight[random_state_column] == r_state) ]\n",
    "                if df_current_run.empty:\n",
    "                    results_individual.append({ 'weight': weight_val, 'dataset': dataset_name, 'random_state': r_state, 'matching_timeUsed': np.nan, 'corresponding_time': np.nan }); continue\n",
    "\n",
    "                current_f1_map = df_current_run.groupby(timeUsed_column_name)[metric_to_compare].mean()\n",
    "                current_time_map = df_current_run.groupby(timeUsed_column_name)[time_column_name].mean()\n",
    "\n",
    "                timeUsed_values_to_check = sorted(current_f1_map.index.unique())\n",
    "                if not timeUsed_values_to_check:\n",
    "                    results_individual.append({ 'weight': weight_val, 'dataset': dataset_name, 'random_state': r_state, 'matching_timeUsed': np.nan, 'corresponding_time': np.nan }); continue\n",
    "\n",
    "\n",
    "                matching_timeUsed = np.nan\n",
    "                corresponding_time = np.nan\n",
    "\n",
    "                # Ciclo sui valori di timeUsed per trovare il match\n",
    "                for t_used in timeUsed_values_to_check:\n",
    "                    f1_d = current_f1_map.get(t_used)\n",
    "                    if f1_d is not None and np.isfinite(f1_d):\n",
    "                       if (f1_d >= current_f1_s_max if MAGGIORE else abs(current_f1_s_max - f1_d) <= TOLLERANCE):\n",
    "                            matching_timeUsed = t_used\n",
    "                            if current_time_map is not None:\n",
    "                                corresponding_time = current_time_map.get(t_used)\n",
    "                            break\n",
    "\n",
    "                if np.isnan(matching_timeUsed):\n",
    "                    max_t_used_for_run = timeUsed_values_to_check[-1]\n",
    "                    matching_timeUsed = max_t_used_for_run\n",
    "                    if current_time_map is not None:\n",
    "                        corresponding_time = current_time_map.get(max_t_used_for_run)\n",
    "\n",
    "                results_individual.append({\n",
    "                    'weight': weight_val,\n",
    "                    'dataset': dataset_name,\n",
    "                    'random_state': r_state,\n",
    "                    'matching_timeUsed': matching_timeUsed,\n",
    "                    'corresponding_time': corresponding_time\n",
    "                })\n",
    "\n",
    "    df_individual = pd.DataFrame(results_individual)\n",
    "    df_average = df_individual.groupby(['weight', 'dataset'])[['matching_timeUsed', 'corresponding_time']].mean().reset_index()\n",
    "\n",
    "    plot_data_y1_avg = df_average.dropna(subset=['matching_timeUsed'])\n",
    "    plot_data_y1_ind = df_individual.dropna(subset=['matching_timeUsed'])\n",
    "\n",
    "    if not plot_data_y1_avg.empty:\n",
    "        fig, ax1 = plt.subplots(figsize=(24, 8))\n",
    "        unique_datasets = df_average['dataset'].unique()\n",
    "        color_palette = sns.color_palette(n_colors=len(unique_datasets))\n",
    "        dataset_colors = dict(zip(unique_datasets, color_palette))\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=plot_data_y1_ind, x='weight', y='matching_timeUsed', hue='dataset',\n",
    "            palette=dataset_colors, units='random_state', estimator=None,\n",
    "            linewidth=1, alpha=0.4, legend=False, ax=ax1\n",
    "        )\n",
    "        sns.lineplot(\n",
    "            data=plot_data_y1_avg, x='weight', y='matching_timeUsed', hue='dataset',\n",
    "            palette=dataset_colors, style='dataset', marker='o', markersize=9, dashes=False,\n",
    "            linewidth=2, legend='full', ax=ax1\n",
    "        )\n",
    "\n",
    "        ax1.set_xlabel(\"Peso\")\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_xticks(PESI)\n",
    "        ax1.set_xticklabels(PESI)\n",
    "        y1_axis_label = f\"timeUsed where F1 >= F1 Max baseline\"\n",
    "        ax1.set_ylabel(y1_axis_label, color='tab:blue')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "        ax1.grid(True, which='major', axis='y', linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "        ax1.grid(True, which='both', axis='x', linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "        ax1.yaxis.set_major_formatter(ticker.FuncFormatter(format_seconds_to_minutes))\n",
    "        ymax = df_singolo_full[df_singolo_full['position'] == position_to_plot]['timeUsed'].max()\n",
    "        ax1.set_ylim(-60, ymax+60)\n",
    "\n",
    "\n",
    "        ax2 = None\n",
    "        plot_data_y2_avg = df_average.dropna(subset=['corresponding_time'])\n",
    "        plot_data_y2_ind = df_individual.dropna(subset=['corresponding_time'])\n",
    "\n",
    "        if not plot_data_y2_avg.empty and TEMPO:\n",
    "            ax2 = ax1.twinx()\n",
    "            sns.lineplot(\n",
    "                data=plot_data_y2_ind, x='weight', y='corresponding_time', hue='dataset',\n",
    "                palette=dataset_colors, units='random_state', estimator=None,\n",
    "                linewidth=1, alpha=0.4, legend=False, ax=ax2, linestyle='dotted'\n",
    "            )\n",
    "            sns.lineplot(\n",
    "                data=plot_data_y2_avg, x='weight', y='corresponding_time', hue='dataset',\n",
    "                palette=dataset_colors, style='dataset', marker='s', markersize=9, dashes=False,\n",
    "                linewidth=2, legend=False, ax=ax2, linestyle='dotted'\n",
    "            )\n",
    "\n",
    "            ax2.set_ylabel(f'Tempo Addestramento Corrispondente ({time_column_name})', color='tab:red')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "            ax2.yaxis.set_major_formatter(ticker.FuncFormatter(format_seconds_to_minutes))\n",
    "\n",
    "        tick_interval_seconds = 5 * 60\n",
    "        yticks = np.arange(0, ymax + tick_interval_seconds, tick_interval_seconds)\n",
    "        ax1.set_yticks(yticks)\n",
    "\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        num_datasets = len(unique_datasets)\n",
    "        handles, labels = handles[:num_datasets], labels[:num_datasets]\n",
    "        ax1.legend(handles, labels, title='Dataset (Media)', bbox_to_anchor=(1.18, 1), loc='upper left')\n",
    "\n",
    "        plt.title(f'Posizione: {position_to_plot}')\n",
    "        fig.tight_layout(rect=[0, 0, 0.82, 1])\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

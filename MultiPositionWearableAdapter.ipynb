{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.091809Z",
     "start_time": "2025-04-28T20:52:08.085228Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.108101Z",
     "start_time": "2025-04-28T20:52:08.102050Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 13\n",
    "SAMPLE_FREQUENCY = round(1000/SAMPLE_RATE, 6)\n",
    "SAMPLE_FREQUENCY_STR = str(SAMPLE_FREQUENCY)\n",
    "script_dir = os.getcwd() + '/data/MultiPositionWearable/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.148477Z",
     "start_time": "2025-04-28T20:52:08.136954Z"
    }
   },
   "outputs": [],
   "source": [
    "def inizializza():\n",
    "    df = pd.read_csv(script_dir + \"raw_data_all.csv\")\n",
    "\n",
    "    for (userid, activity, position), group in df.groupby([\"Userid\", \"Activity\", \"position\"]):\n",
    "        user_folder = f\"User{userid}\"\n",
    "        user_folder_path = os.path.join(script_dir, user_folder)\n",
    "\n",
    "        os.makedirs(user_folder_path, exist_ok=True)  # <-- crea X/data/UserX/\n",
    "\n",
    "        filename = f\"User{userid}_{activity}_Sensor-{position}.csv\"\n",
    "        filepath = os.path.join(user_folder_path, filename)  # <-- salva dentro UserX/\n",
    "\n",
    "        group.to_csv(filepath, index=False)\n",
    "\n",
    "    print(\"Tutto ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.222536Z",
     "start_time": "2025-04-28T20:52:08.214370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import data as nested dictionary\n",
    "def get_data(df_dict, user):\n",
    "    current_user = user\n",
    "    mypath = script_dir + current_user + '/'\n",
    "    file_list = [f for f in listdir(mypath) if isfile(join(mypath, f)) and re.compile(current_user).match(f)]\n",
    "    activites = set([file.split('_')[1] for file in file_list])\n",
    "    sensors = set([file.split('_')[2].split('-')[1].split('.')[0] for file in file_list])\n",
    "    df_dict = {activity: {sensor: pd.DataFrame() for sensor in sensors} for activity in activites}\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(mypath + file)\n",
    "\n",
    "        # check for nan values\n",
    "        nan_values = df.iloc[:, :17].isnull().values.any()\n",
    "        if nan_values:\n",
    "            df1 = df[df.isna().any(axis=1)]\n",
    "            print(\"File: \" + file + \" has nan values\\nDataFrame:\")\n",
    "        activity = file.split('_')[1]\n",
    "        sensor = file.split('_')[2].split('-')[1].split('.')[0]\n",
    "        df_dict[activity][sensor] = \\\n",
    "            pd.concat([df_dict[activity][sensor], df]).reset_index(drop=True)\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.248427Z",
     "start_time": "2025-04-28T20:52:08.237701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(df_dict):\n",
    "    for activity, df_activity in df_dict.items():\n",
    "        for sensor, df_sensor in df_activity.items():\n",
    "            discarded_columns = \\\n",
    "            pd.concat([df_sensor.iloc[:, 0:6], df_sensor.iloc[:, 16]], axis=1)\n",
    "            # Convert timestamps to DateTime format\n",
    "            #display(df_sensor.columns.tolist())\n",
    "            df_sensor['Timestamp'] = \\\n",
    "                pd.to_datetime(\n",
    "                    df_sensor['UTC_start-end'][0] \\\n",
    "                        + (df_sensor['Timestamp']\\\n",
    "                        -df_sensor['relativeTime_start-end'][0])*1000, \n",
    "                            unit='us')\n",
    "            # Align all dataframes to the same timestamps\n",
    "            df_sensor = df_sensor.iloc[:,6:16].resample(SAMPLE_FREQUENCY_STR+'ms', on='Timestamp').mean().dropna().reset_index()\n",
    "            #if (df_sensor.isnull().values.any()):\n",
    "                #print(\"Activity: \" + activity + \" Sensor: \" + sensor + \" has nan values\")\n",
    "                #display(df_sensor[df_sensor.isna().any(axis=1)])\n",
    "            # Reindex discarded_columns to match the length of df_sensor\n",
    "            discarded_columns = discarded_columns.reindex(df_sensor.index)\n",
    "            # Add back the columns that were discarded\n",
    "            df_sensor = pd.concat([df_sensor, discarded_columns], axis=1).dropna().reset_index(drop=True)\n",
    "            # Save the modified dataframe\n",
    "            df_activity[sensor] = df_sensor\n",
    "\n",
    "        # Align first and last probes for all the dataframes\n",
    "        initial_timestamps = [df_sensor['Timestamp'].iloc[0] for df_sensor in df_activity.values()]\n",
    "        final_timestamps = [df_sensor['Timestamp'].iloc[-1] for df_sensor in df_activity.values()]\n",
    "        max_initial_timestamp = max(initial_timestamps)\n",
    "        min_final_timestamp = min(final_timestamps)\n",
    "        for sensor, df_sensor in df_activity.items():\n",
    "            # Discard data that is not within the range of the first and last probe\n",
    "            df_sensor = df_sensor[(df_sensor['Timestamp'] > max_initial_timestamp) & (df_sensor['Timestamp'] < min_final_timestamp)]\n",
    "            df_activity[sensor] = df_sensor.reset_index(drop=True)\n",
    "    return df_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Data With Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.284511Z",
     "start_time": "2025-04-28T20:52:08.275416Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frames(data, frame_size, hop_size):\n",
    "    r = np.arange(len(data))   \n",
    "    s = r[::hop_size]   \n",
    "    z = list(zip(s, s + frame_size))   \n",
    "    g = lambda hop_size: data.iloc[hop_size[0]:hop_size[1]]   \n",
    "    return pd.concat(map(g, z), keys=range(len(z)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate magnitude between data from the same IMU sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.323457Z",
     "start_time": "2025-04-28T20:52:08.313312Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_magnitude(df_dict):\n",
    "    for activity, df_activity in df_dict.items():\n",
    "        for sensor, df_sensor in df_activity.items():\n",
    "            df_sensor = pd.concat([\n",
    "                df_sensor,\n",
    "                np.sqrt(np.square(df_sensor[['AccX', 'AccY', 'AccZ']]).sum(axis=1)).rename(\"AccMagnitude\"),\n",
    "                np.sqrt(np.square(df_sensor[['GyroX', 'GyroY', 'GyroZ']]).sum(axis=1)).rename(\"GyroMagnitude\"),\n",
    "                np.sqrt(np.square(df_sensor[['MagnX', 'MagnY','MagnZ']]).sum(axis=1)).rename(\"MagnMagnitude\"),\n",
    "            ], axis=1)\n",
    "            df_activity[sensor] = df_sensor\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.369782Z",
     "start_time": "2025-04-28T20:52:08.364344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge raw data in single dataframe\n",
    "def get_raw_data(df_dict):\n",
    "    df_raw_data = pd.DataFrame()\n",
    "    for activity, df_activity in df_dict.items():\n",
    "        for sensor, df_sensor in df_activity.items():\n",
    "            df_raw_data = pd.concat([df_raw_data, df_sensor]).reset_index(drop=True)\n",
    "    return df_raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate 1% Range Over/In/Below Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.450887Z",
     "start_time": "2025-04-28T20:52:08.442493Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_over_in_below_mean(df, perc=0.01):\n",
    "    # Create an empty dataframe to store the results\n",
    "    result_df = pd.DataFrame()\n",
    "    df_mean = df.mean()\n",
    "\n",
    "    # Calculate the lower and upper limits\n",
    "    df_lim_inf = df_mean.apply(lambda x: x - x*perc)\n",
    "    df_lim_sup = df_mean.apply(lambda x: x + x*perc)\n",
    "\n",
    "    # Create a new dataframe to count the number of values over, in and below the mean\n",
    "    for col in df.columns:\n",
    "        result_df[col + 'OverMean'] = pd.Series((df[col] > df_lim_sup[col]).value_counts().get(True, 0))\n",
    "        result_df[col + 'InMean'] = pd.Series(((df[col] >= df_lim_inf[col]) & (df[col] <= df_lim_sup[col])).value_counts().get(True, 0))\n",
    "        result_df[col + 'BelowMean'] = pd.Series((df[col] < df_lim_inf[col]).value_counts().get(True, 0))\n",
    "        \n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data in Single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.475958Z",
     "start_time": "2025-04-28T20:52:08.468260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge all the dataframes into a single dataframe\n",
    "def merge_data(df_dict):\n",
    "    df_data = pd.DataFrame()\n",
    "    for df_activity in df_dict.values():\n",
    "        for df_sensor in df_activity.values():\n",
    "            df_data = pd.concat([df_data, df_sensor])\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "* STD\n",
    "* avg\n",
    "* min\n",
    "* max\n",
    "* above/in/below range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.513852Z",
     "start_time": "2025-04-28T20:52:08.506128Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_fft_energy(frame, signal_len, df_energy):\n",
    "    fft_result = np.fft.fft(frame)\n",
    "    power_spectrum = np.abs(fft_result)**2\n",
    "    power_spectrum /= signal_len\n",
    "    energy = np.sum(power_spectrum, axis=0)\n",
    "    energy = pd.DataFrame([energy], columns=df_energy.columns)\n",
    "    df_energy = pd.concat([df_energy, energy], ignore_index=True)\n",
    "    return df_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:52:08.561960Z",
     "start_time": "2025-04-28T20:52:08.546622Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_size = SAMPLE_RATE*4  # Window:  4 seconds\n",
    "hop_size = SAMPLE_RATE*2  \n",
    "\n",
    "# Calculate features for each frame\n",
    "def calculate_features(df_dict):\n",
    "  # Overlap: 50%\n",
    "    for df_activity in df_dict.values():\n",
    "        for sensor, df_sensor in df_activity.items():\n",
    "            df_energy = pd.DataFrame(columns=[\n",
    "                \"AccxEnergy\", \"AccyEnergy\", \"AcczEnergy\", \n",
    "                \"GyroxEnergy\", \"GyroyEnergy\", \"GyrozEnergy\", \n",
    "                \"MagnxEnergy\", \"MagnyEnergy\", \"MagnzEnergy\", \n",
    "                \"AccMagnitudeEnergy\", \"GyroMagnitudeEnergy\", \"MagnMagnitudeEnergy\"\n",
    "            ])\n",
    "            # Get frames\n",
    "            df_sensor = get_frames(df_sensor, frame_size, hop_size)\n",
    "            # Save columns that are going to be discarded\n",
    "            discarded_columns = pd.concat([df_sensor.iloc[:, 0], df_sensor.iloc[:, 10:-3]], axis=1).reset_index(drop=True)\n",
    "            # Discard columns that are not going to be used for grouping\n",
    "            df_sensor = df_sensor[['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ', 'MagnX', 'MagnY', 'MagnZ', 'AccMagnitude', 'GyroMagnitude', 'MagnMagnitude']]\n",
    "            # Calculate Energy Spectral Density for each frame and separately for each axis\n",
    "            df_energy = df_sensor.groupby(level=0).apply(lambda x: calculate_fft_energy(x, frame_size, df_energy)).reset_index(drop=True)\n",
    "            # Calculate mean for each frame\n",
    "            df_mean = df_sensor.groupby(level=0).mean().reset_index(drop=True)\n",
    "            df_mean.columns = [col + 'Mean' for col in df_mean.columns]\n",
    "            # Calculate std\n",
    "            df_std = df_sensor.groupby(level=0).std().reset_index(drop=True)\n",
    "            df_std.columns = [col + 'Std' for col in df_std.columns]\n",
    "            # Calculate min\n",
    "            df_min = df_sensor.groupby(level=0).min().reset_index(drop=True)\n",
    "            df_min.columns = [col + 'Min' for col in df_min.columns]\n",
    "            # Calculate max\n",
    "            df_max = df_sensor.groupby(level=0).max().reset_index(drop=True)\n",
    "            df_max.columns = [col + 'Max' for col in df_max.columns]\n",
    "            # Calculate number of values in, over and below 1% of the mean\n",
    "            df_over_in_below_mean = df_sensor.groupby(level=0).apply(lambda x: calc_over_in_below_mean(x)).reset_index(drop=True)\n",
    "            # Add back the columns that were discarded and the calculated energy\n",
    "            df_activity[sensor] = pd.concat([df_mean, discarded_columns, df_energy, df_std, df_min, df_max, df_over_in_below_mean], axis=1).dropna().reset_index(drop=True)\n",
    "\n",
    "    df_data = merge_data(df_dict)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write grouped data and raw data on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:57:58.466320Z",
     "start_time": "2025-04-28T20:52:08.631440Z"
    }
   },
   "outputs": [],
   "source": [
    "users = [\n",
    "     'User0',\n",
    "     'User1',\n",
    "     'User2',\n",
    "     'User3',\n",
    "     'User4',\n",
    "     'User5',\n",
    "     'User6',\n",
    "     'User7',\n",
    "     'User8',\n",
    "     'User9',\n",
    " ]\n",
    "\n",
    "inizializza()\n",
    "for user in tqdm(users):\n",
    "    df_dict = {}\n",
    "    df_dict = get_data(df_dict, user)\n",
    "    #df_dict = preprocess_data(df_dict)\n",
    "    df_raw_data = get_raw_data(df_dict) #faccio nulla\n",
    "    df_dict = calculate_magnitude(df_dict)\n",
    "    df_data = calculate_features(df_dict)\n",
    "    # Save the data\n",
    "    os.makedirs(script_dir + \"MultiPositionWearable_processed_data\", exist_ok=True)\n",
    "    df_data.to_csv(script_dir + \"MultiPositionWearable_processed_data/\" + 'grouped_data_' + user + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUp():\n",
    "  for item in os.listdir(script_dir):\n",
    "    item_path = os.path.join(script_dir, item)\n",
    "    \n",
    "    if os.path.isdir(item_path) and re.fullmatch(r'User\\d+', item):\n",
    "      shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanUp();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
